{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8ncdA9HdI8k-",
        "outputId": "329f19ca-e4d8-4abe-fae6-c6a5aa022bda"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: gdown in /usr/local/lib/python3.12/dist-packages (5.2.0)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.12/dist-packages (from gdown) (4.13.5)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from gdown) (3.20.0)\n",
            "Requirement already satisfied: requests[socks] in /usr/local/lib/python3.12/dist-packages (from gdown) (2.32.4)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from gdown) (4.67.1)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.12/dist-packages (from beautifulsoup4->gdown) (2.8)\n",
            "Requirement already satisfied: typing-extensions>=4.0.0 in /usr/local/lib/python3.12/dist-packages (from beautifulsoup4->gdown) (4.15.0)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests[socks]->gdown) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests[socks]->gdown) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests[socks]->gdown) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests[socks]->gdown) (2025.11.12)\n",
            "Requirement already satisfied: PySocks!=1.5.7,>=1.5.6 in /usr/local/lib/python3.12/dist-packages (from requests[socks]->gdown) (1.7.1)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (4.67.1)\n",
            "Collecting ultralytics\n",
            "  Downloading ultralytics-8.3.247-py3-none-any.whl.metadata (37 kB)\n",
            "Requirement already satisfied: numpy>=1.23.0 in /usr/local/lib/python3.12/dist-packages (from ultralytics) (2.0.2)\n",
            "Requirement already satisfied: matplotlib>=3.3.0 in /usr/local/lib/python3.12/dist-packages (from ultralytics) (3.10.0)\n",
            "Requirement already satisfied: opencv-python>=4.6.0 in /usr/local/lib/python3.12/dist-packages (from ultralytics) (4.12.0.88)\n",
            "Requirement already satisfied: pillow>=7.1.2 in /usr/local/lib/python3.12/dist-packages (from ultralytics) (11.3.0)\n",
            "Requirement already satisfied: pyyaml>=5.3.1 in /usr/local/lib/python3.12/dist-packages (from ultralytics) (6.0.3)\n",
            "Requirement already satisfied: requests>=2.23.0 in /usr/local/lib/python3.12/dist-packages (from ultralytics) (2.32.4)\n",
            "Requirement already satisfied: scipy>=1.4.1 in /usr/local/lib/python3.12/dist-packages (from ultralytics) (1.16.3)\n",
            "Requirement already satisfied: torch>=1.8.0 in /usr/local/lib/python3.12/dist-packages (from ultralytics) (2.9.0+cu126)\n",
            "Requirement already satisfied: torchvision>=0.9.0 in /usr/local/lib/python3.12/dist-packages (from ultralytics) (0.24.0+cu126)\n",
            "Requirement already satisfied: psutil>=5.8.0 in /usr/local/lib/python3.12/dist-packages (from ultralytics) (5.9.5)\n",
            "Requirement already satisfied: polars>=0.20.0 in /usr/local/lib/python3.12/dist-packages (from ultralytics) (1.31.0)\n",
            "Collecting ultralytics-thop>=2.0.18 (from ultralytics)\n",
            "  Downloading ultralytics_thop-2.0.18-py3-none-any.whl.metadata (14 kB)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.3.0->ultralytics) (1.3.3)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.3.0->ultralytics) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.3.0->ultralytics) (4.61.1)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.3.0->ultralytics) (1.4.9)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.3.0->ultralytics) (25.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.3.0->ultralytics) (3.2.5)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.3.0->ultralytics) (2.9.0.post0)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests>=2.23.0->ultralytics) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests>=2.23.0->ultralytics) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests>=2.23.0->ultralytics) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests>=2.23.0->ultralytics) (2025.11.12)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (3.20.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (4.15.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (1.14.0)\n",
            "Requirement already satisfied: networkx>=2.5.1 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (3.6.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (3.1.6)\n",
            "Requirement already satisfied: fsspec>=0.8.5 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (2025.3.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.5 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (2.27.5)\n",
            "Requirement already satisfied: nvidia-nvshmem-cu12==3.3.20 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (3.3.20)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.5.0 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (3.5.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.7->matplotlib>=3.3.0->ultralytics) (1.17.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch>=1.8.0->ultralytics) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch>=1.8.0->ultralytics) (3.0.3)\n",
            "Downloading ultralytics-8.3.247-py3-none-any.whl (1.2 MB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m27.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading ultralytics_thop-2.0.18-py3-none-any.whl (28 kB)\n",
            "Installing collected packages: ultralytics-thop, ultralytics\n",
            "Successfully installed ultralytics-8.3.247 ultralytics-thop-2.0.18\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "['/content/train/reid_checkpoints/phase1_reid_hybrid_ENDZONE_epoch_20.pth',\n",
              " '/content/train/reid_checkpoints/phase1_reid_hybrid_SIDELINE_epoch_20.pth',\n",
              " '/content/train/best_dualview_96.pth',\n",
              " '/content/train/best_siamese_matcher_endzone.pth',\n",
              " '/content/train/best_siamese_matcher.pth',\n",
              " '/content/train/best_weights_players_detection.pt',\n",
              " '/content/train/best.pt',\n",
              " '/content/train/ckpt.t7',\n",
              " '/content/train/csv_helmets.zip',\n",
              " '/content/train/embeddings.zip',\n",
              " '/content/train/helmet_dataset.yaml',\n",
              " '/content/train/helmets_extracted.zip',\n",
              " '/content/train/reid_gt_metadata_ENDZONE.csv',\n",
              " '/content/train/reid_gt_metadata_SIDELINE.csv',\n",
              " '/content/train/reid_gt_metadata.csv',\n",
              " '/content/train/train_player_tracking_frames.csv',\n",
              " '/content/train/video_directions.csv',\n",
              " '/content/train/Videos_dataset.zip']"
            ]
          },
          "execution_count": 1,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "!pip install gdown\n",
        "!pip install tqdm\n",
        "!pip install ultralytics\n",
        "\n",
        "\n",
        "import gdown         \n",
        "import os\n",
        "import shutil\n",
        "import zipfile\n",
        "from google.colab import files \n",
        "import random\n",
        "import math\n",
        "\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from collections import defaultdict\n",
        "from collections import deque\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torchvision.models.video import r2plus1d_18, R2Plus1D_18_Weights\n",
        "import torch.nn.functional as F\n",
        "import torchvision.models as models\n",
        "from torch.utils.data import Dataset, WeightedRandomSampler, DataLoader, SubsetRandomSampler\n",
        "from torchvision import transforms\n",
        "import torch.optim as optim\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "import cv2\n",
        "from tqdm import tqdm \n",
        "from PIL import Image\n",
        "from IPython.display import display, FileLink, Markdown\n",
        "from collections import defaultdict\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "from scipy.optimize import linear_sum_assignment\n",
        "import matplotlib.pyplot as plt\n",
        "import torchvision.transforms as T\n",
        "import torch.cuda.amp as amp\n",
        "\n",
        "\n",
        "url_videos = \"https://drive.google.com/drive/folders/1nO_javaDEk0yFcglOX7WoqRNj_MtCEDd?usp=drive_link\"\n",
        "gdown.download_folder(url_videos, quiet=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5rzCFOj0I9cq",
        "outputId": "2755661d-7a8e-400a-874c-6667b9ab274c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úÖ Cartella 'train' spostata in 'datasets/Dataset'\n",
            "File zip datasets/Dataset/helmets_extracted.zip estratto e rimosso.\n",
            "File zip datasets/Dataset/Videos_dataset.zip estratto e rimosso.\n",
            "File zip datasets/Dataset/csv_helmets.zip estratto e rimosso.\n",
            "File zip datasets/Dataset/embeddings.zip estratto e rimosso.\n"
          ]
        }
      ],
      "source": [
        "\n",
        "original_path = \"train\"\n",
        "new_base = \"datasets\"\n",
        "new_path = os.path.join(new_base, \"Dataset\")\n",
        "\n",
        "os.makedirs(new_base, exist_ok=True)\n",
        "\n",
        "if not os.path.exists(new_path):\n",
        "    shutil.move(original_path, new_path)\n",
        "    print(f\" Folder '{original_path}' moved in '{new_path}'\")\n",
        "dataset_path = \"datasets/Dataset\"\n",
        "\n",
        "# extract zip files and remove them\n",
        "def extract_and_remove_zip(zip_path, extract_to):\n",
        "    with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
        "        zip_ref.extractall(extract_to)\n",
        "    os.remove(zip_path)\n",
        "    print(f\"File zip {zip_path} extracted and removed.\")\n",
        "\n",
        "zip_files = [f for f in os.listdir(dataset_path) if f.endswith('.zip')]\n",
        "for zip_file in zip_files:\n",
        "    zip_path = os.path.join(dataset_path, zip_file)\n",
        "    extract_and_remove_zip(zip_path, dataset_path)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "abQwMHNsRSWE"
      },
      "source": [
        "#**DATASET PREPROCESSING**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "raJgE-dgI9x4",
        "outputId": "e7a57979-9c88-4d6e-9925-12630815da7e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚öôÔ∏è Configurazione impostata per visuale: Endzone\n",
            "üö´ Esclusione di 6 video di test dal dataset di training.\n",
            "\n",
            "üî¨ Inizio estrazione SOLO per Endzone...\n",
            "   (Include fix NaN e Tolleranza Buchi Temporali)\n",
            "   ‚úÇÔ∏è Rimossi 56230 record appartenenti ai video di test.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Processing Endzone Tracks: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1168/1168 [02:14<00:00,  8.67it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "‚úÖ Estrazione Endzone completata. Totale sottotracce: 101881\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "FRAME_W = 1280      \n",
        "FRAME_H = 720       \n",
        "IMPACT_LABELS_PATH = \"/content/datasets/Dataset/Videos_dataset/train_labels.csv\"\n",
        "\n",
        "# SELECT VIEW ('Endzone' or 'Sideline')\n",
        "TARGET_VIEW = 'Endzone'\n",
        "\n",
        "# VIDEOS FOR TESTING\n",
        "TEST_VIDEOS = [\n",
        "    \"57906_000718_Endzone.mp4\", \"57906_000718_Sideline.mp4\",\n",
        "    \"57995_000109_Endzone.mp4\", \"57995_000109_Sideline.mp4\",\n",
        "    \"58102_002798_Endzone.mp4\", \"58102_002798_Sideline.mp4\",\n",
        "]\n",
        "\n",
        "\n",
        "\n",
        "class VideoAugmentationPipeline:\n",
        "    def __init__(self, p_occlusion=0.4, p_switch=0.3):\n",
        "        self.p_occlusion = p_occlusion\n",
        "        self.p_switch = p_switch\n",
        "\n",
        "        self.stats = {\n",
        "            'total_sequences': 0,\n",
        "            'clean': 0,\n",
        "            'occlusion_applied': 0,\n",
        "            'id_switch_applied': 0,\n",
        "            'both_applied': 0\n",
        "        }\n",
        "\n",
        "    def __call__(self, images):\n",
        "        self.stats['total_sequences'] += 1\n",
        "        has_occlusion = False\n",
        "        has_switch = False\n",
        "\n",
        "        if len(images) > 4 and random.random() < self.p_occlusion:\n",
        "            num_frames_to_drop = random.randint(1, 3)\n",
        "            indices_to_drop = random.sample(range(len(images)), num_frames_to_drop)\n",
        "\n",
        "            w, h = images[0].size\n",
        "            black_frame = Image.new('RGB', (w, h), (0, 0, 0))\n",
        "\n",
        "            for idx in indices_to_drop:\n",
        "                images[idx] = black_frame\n",
        "\n",
        "            has_occlusion = True\n",
        "\n",
        "        if len(images) > 6 and random.random() < self.p_switch:\n",
        "            mode = random.choice(['start', 'end'])\n",
        "            block_size = random.randint(4, 6)\n",
        "\n",
        "            w, h = images[0].size\n",
        "            black_frame = Image.new('RGB', (w, h), (0, 0, 0))\n",
        "\n",
        "            if mode == 'end':\n",
        "                for i in range(len(images) - block_size, len(images)):\n",
        "                    images[i] = black_frame\n",
        "            else:\n",
        "                for i in range(block_size):\n",
        "                    images[i] = black_frame\n",
        "\n",
        "            has_switch = True\n",
        "\n",
        "        if has_occlusion and has_switch:\n",
        "            self.stats['both_applied'] += 1\n",
        "        elif has_occlusion:\n",
        "            self.stats['occlusion_applied'] += 1\n",
        "        elif has_switch:\n",
        "            self.stats['id_switch_applied'] += 1\n",
        "        else:\n",
        "            self.stats['clean'] += 1\n",
        "\n",
        "        return images\n",
        "\n",
        "    def get_and_reset_stats(self):\n",
        "        if self.stats['total_sequences'] == 0:\n",
        "            return \"No data processed.\"\n",
        "\n",
        "        total = self.stats['total_sequences']\n",
        "        report = (\n",
        "            f\" REPORT AUGMENTATION (on {total} sequences):\\n\"\n",
        "            f\"   - Cleared: {self.stats['clean']} ({self.stats['clean']/total:.1%})\\n\"\n",
        "            f\"   - Only Occlusion : {self.stats['occlusion_applied']} ({self.stats['occlusion_applied']/total:.1%})\\n\"\n",
        "            f\"   - Only ID Switch : {self.stats['id_switch_applied']} ({self.stats['id_switch_applied']/total:.1%})\\n\"\n",
        "            f\"   - Extreme: {self.stats['both_applied']} ({self.stats['both_applied']/total:.1%})\"\n",
        "        )\n",
        "\n",
        "        # Reset\n",
        "        self.stats = {k: 0 for k in self.stats}\n",
        "        return report\n",
        "\n",
        "def get_train_transforms():\n",
        "    return T.Compose([\n",
        "        T.RandomApply([T.Resize((48, 48)), T.Resize((64, 64))], p=0.2),\n",
        "        T.RandomApply([T.GaussianBlur(kernel_size=3, sigma=(0.1, 2.0))], p=0.3),\n",
        "        T.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.05),\n",
        "        T.ToTensor(),\n",
        "        T.Normalize(mean=[0.43216, 0.394666, 0.37645], std=[0.22803, 0.22145, 0.216989])\n",
        "    ])\n",
        "\n",
        "def get_val_transforms():\n",
        "    return T.Compose([\n",
        "        T.ToTensor(),\n",
        "        T.Normalize(mean=[0.43216, 0.394666, 0.37645], std=[0.22803, 0.22145, 0.216989])\n",
        "    ])\n",
        "\n",
        "\n",
        "def convert_to_tlbr(df):\n",
        "    df['x1'] = df['left']\n",
        "    df['y1'] = df['top']\n",
        "    df['x2'] = df['left'] + df['width']\n",
        "    df['y2'] = df['top'] + df['height']\n",
        "    return df\n",
        "\n",
        "def bbox_iou_raw(boxA, boxB):\n",
        "    x1 = max(boxA[0], boxB[0])\n",
        "    y1 = max(boxA[1], boxB[1])\n",
        "    x2 = min(boxA[2], boxB[2])\n",
        "    y2 = min(boxA[3], boxB[3])\n",
        "    interArea = max(0, x2 - x1) * max(0, y2 - y1)\n",
        "    areaA = (boxA[2] - boxA[0]) * (boxA[3] - boxA[1])\n",
        "    areaB = (boxB[2] - boxB[0]) * (boxB[3] - boxB[1])\n",
        "    union = float(areaA + areaB - interArea)\n",
        "    return interArea / union if union > 0 else 0.0\n",
        "\n",
        "def extract_sliding_subsequences(file_path, target_view):\n",
        "\n",
        "    SAMPLE_LENGTH = 16\n",
        "    FRAME_STRIDE = 4\n",
        "    MAX_SPAN_ALLOWED = 25\n",
        "\n",
        "    df = pd.read_csv(file_path)\n",
        "    df = df[~df['video'].isin(TEST_VIDEOS)].copy() \n",
        "\n",
        "    df = df[df['view'] == target_view].copy() \n",
        "    df['impact'] = df['impact'].fillna(0)\n",
        "    df['label'] = df['label'].astype(str)\n",
        "    df = convert_to_tlbr(df)\n",
        "\n",
        "    if 'label' in df.columns and 'player_id' not in df.columns:\n",
        "        df.rename(columns={'label': 'player_id'}, inplace=True)\n",
        "\n",
        "    all_subsequences = []\n",
        "    track_groups = df.groupby(['gameKey', 'playID', 'view', 'player_id'])\n",
        "\n",
        "    for (gameKey, playID, view, player_id), track_df_full in tqdm(track_groups, desc=f\"Processing {target_view} Tracks\"):\n",
        "        track_df_full = track_df_full.sort_values('frame').reset_index(drop=True)\n",
        "        frames = track_df_full['frame'].values\n",
        "\n",
        "        if len(frames) < SAMPLE_LENGTH: continue\n",
        "\n",
        "        for i in range(0, len(frames) - SAMPLE_LENGTH + 1, FRAME_STRIDE):\n",
        "            sub_track_df = track_df_full.iloc[i : i + SAMPLE_LENGTH].copy()\n",
        "\n",
        "            start_frame_val = sub_track_df['frame'].iloc[0]\n",
        "            end_frame_val = sub_track_df['frame'].iloc[-1]\n",
        "            if (end_frame_val - start_frame_val) > MAX_SPAN_ALLOWED: continue\n",
        "\n",
        "            impact_vector = sub_track_df['impact'].values.astype(np.float32)\n",
        "            is_impact_sequence = (np.sum(impact_vector) > 0.5)\n",
        "\n",
        "            video_name = sub_track_df['video'].iloc[0]\n",
        "            image_meta = []\n",
        "            for _, row in sub_track_df.iterrows():\n",
        "                image_meta.append({\n",
        "                    'video_folder': video_name,\n",
        "                    'file_name': f\"{row['player_id']}_{row['frame']}.jpg\",\n",
        "                    'frame_idx': row['frame'],\n",
        "                    'bbox': [row['left'], row['top'], row['width'], row['height']]\n",
        "                })\n",
        "\n",
        "            all_subsequences.append({\n",
        "                'key': f\"{gameKey}_{playID}_{view}_{player_id}_{start_frame_val}\",\n",
        "                'is_impact': is_impact_sequence,\n",
        "                'data': image_meta,\n",
        "                'targets': impact_vector\n",
        "            })\n",
        "\n",
        "    print(f\"\\n Extraction {target_view} completed. Total subtracks: {len(all_subsequences)}\")\n",
        "    return all_subsequences\n",
        "\n",
        "data_samples = extract_sliding_subsequences(IMPACT_LABELS_PATH, TARGET_VIEW)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wq5qxP0XkoFw"
      },
      "outputs": [],
      "source": [
        "class NFLImpactDataset(Dataset):\n",
        "    def __init__(self, samples, root_dir, phase='train', transform=None, video_augmenter=None, sigma=2):\n",
        "        self.samples = samples\n",
        "        self.root_dir = root_dir\n",
        "        self.phase = phase\n",
        "        self.transform = transform\n",
        "        self.video_augmenter = video_augmenter\n",
        "        self.sigma = sigma\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.samples)\n",
        "\n",
        "    def generate_gaussian_target(self, impact_vector, length=16):\n",
        "        \n",
        "        impact_vector = np.nan_to_num(np.array(impact_vector), nan=0.0)\n",
        "        impact_indices = np.where(impact_vector > 0.5)[0]\n",
        "        target = np.zeros(length, dtype=np.float32)\n",
        "\n",
        "        if len(impact_indices) == 0:\n",
        "            return torch.tensor(target, dtype=torch.float32)\n",
        "\n",
        "        x = np.arange(length)\n",
        "        for idx in impact_indices:\n",
        "            gauss = np.exp(-((x - idx)**2) / (2 * self.sigma**2))\n",
        "            target = np.maximum(target, gauss)\n",
        "\n",
        "        return torch.tensor(target, dtype=torch.float32)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        sample = self.samples[idx]\n",
        "        meta_data = sample['data']\n",
        "\n",
        "        images = []\n",
        "\n",
        "        for item in meta_data:\n",
        "            video_folder = item['video_folder'].replace('.mp4', '')\n",
        "            img_path = os.path.join(self.root_dir, video_folder, item['file_name'])\n",
        "\n",
        "            try:\n",
        "                img = Image.open(img_path).convert('RGB')\n",
        "                if img.size != (64, 64):\n",
        "                    img = img.resize((64, 64))\n",
        "            except Exception:\n",
        "                img = Image.new('RGB', (64, 64), (0, 0, 0))\n",
        "\n",
        "            images.append(img)\n",
        "\n",
        "        # VIDEO AUGMENTATION (Masking/Occlusions)\n",
        "        if self.phase == 'train' and self.video_augmenter:\n",
        "            images = self.video_augmenter(images)\n",
        "\n",
        "        # IMAGE TRANSFORMS (Blur, Color, Tensor, Norm)\n",
        "        processed_frames = []\n",
        "        do_flip = (self.phase == 'train' and random.random() > 0.5)\n",
        "\n",
        "        for img in images:\n",
        "            if do_flip:\n",
        "                img = img.transpose(Image.FLIP_LEFT_RIGHT)\n",
        "\n",
        "            if self.transform:\n",
        "                img_t = self.transform(img)\n",
        "            else:\n",
        "                img_t = T.ToTensor()(img)\n",
        "\n",
        "            processed_frames.append(img_t)\n",
        "\n",
        "        video_tensor = torch.stack(processed_frames).permute(1, 0, 2, 3)\n",
        "\n",
        "        raw_targets = sample['targets']\n",
        "        smooth_targets = self.generate_gaussian_target(raw_targets)\n",
        "\n",
        "        return video_tensor, smooth_targets"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DlurbIoCWJPq"
      },
      "source": [
        "#**Model Definition**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CKFmAqFWqvvn"
      },
      "outputs": [],
      "source": [
        "class NFLImpactModel(nn.Module):\n",
        "    def __init__(self, num_frames=16, hidden_dim=256, dropout_prob=0.5):\n",
        "        super(NFLImpactModel, self).__init__()\n",
        "\n",
        "        weights = R2Plus1D_18_Weights.DEFAULT\n",
        "        self.backbone = r2plus1d_18(weights=weights)\n",
        "\n",
        "        in_features = self.backbone.fc.in_features\n",
        "\n",
        "        self.backbone.fc = nn.Identity()\n",
        "\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Linear(in_features, hidden_dim),\n",
        "            nn.BatchNorm1d(hidden_dim), \n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(p=dropout_prob), \n",
        "            nn.Linear(hidden_dim, num_frames) \n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        features = self.backbone(x) # Output: (Batch, 512)\n",
        "\n",
        "        out = self.classifier(features) # Output: (Batch, 16)\n",
        "\n",
        "        return out"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nsLFwn0Xr5oT"
      },
      "source": [
        "#**Training Phase**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SQnOXuTZrAMX",
        "outputId": "c90367e4-5f95-4f14-eb71-23fe4ae90628"
      },
      "outputs": [],
      "source": [
        "print(f\"START SETUP TRAINING FOR: {TARGET_VIEW.upper()}\")\n",
        "\n",
        "\n",
        "all_video_names = list(set([s['data'][0]['video_folder'] for s in data_samples]))\n",
        "print(f\"Total Videos for training phase: {len(all_video_names)}\")\n",
        "\n",
        "train_videos, val_videos = train_test_split(all_video_names, test_size=0.2, random_state=42)\n",
        "\n",
        "print(f\"   -> Videos in Train: {len(train_videos)}\")\n",
        "print(f\"   -> Videos in Val:   {len(val_videos)}\")\n",
        "\n",
        "train_samples_all = [s for s in data_samples if s['data'][0]['video_folder'] in train_videos]\n",
        "val_samples_all = [s for s in data_samples if s['data'][0]['video_folder'] in val_videos]\n",
        "\n",
        "train_pos = [s for s in train_samples_all if np.sum(s['targets']) > 0.5]\n",
        "train_neg = [s for s in train_samples_all if np.sum(s['targets']) < 0.5]\n",
        "\n",
        "val_pos = [s for s in val_samples_all if np.sum(s['targets']) > 0.5]\n",
        "val_neg = [s for s in val_samples_all if np.sum(s['targets']) < 0.5]\n",
        "\n",
        "print(f\"   TRAIN -> Positives: {len(train_pos)} | Negatives: {len(train_neg)}\")\n",
        "print(f\"   VAL   -> Positives: {len(val_pos)}   | Negatives: {len(val_neg)}\")\n",
        "\n",
        "train_samples_final = train_pos + train_neg\n",
        "val_samples_final = val_pos + val_neg\n",
        "\n",
        "train_dataset = NFLImpactDataset(\n",
        "    train_samples_final,\n",
        "    root_dir=\"/content/datasets/Dataset/helmets_extracted\",\n",
        "    phase='train',\n",
        "    transform=get_train_transforms(),\n",
        "    video_augmenter=VideoAugmentationPipeline(p_occlusion=0.4, p_switch=0.3)\n",
        ")\n",
        "\n",
        "val_dataset = NFLImpactDataset(\n",
        "    val_samples_final,\n",
        "    root_dir=\"/content/datasets/Dataset/helmets_extracted\",\n",
        "    phase='val',\n",
        "    transform=get_val_transforms()\n",
        ")\n",
        "\n",
        "train_targets = [1 if np.sum(s['targets']) > 0.5 else 0 for s in train_samples_final]\n",
        "class_counts = np.bincount(train_targets)\n",
        "weights = 1.0 / np.maximum(class_counts, 1)\n",
        "samples_weights = torch.DoubleTensor([weights[t] for t in train_targets])\n",
        "\n",
        "train_sampler = WeightedRandomSampler(weights=samples_weights, num_samples=15000, replacement=True)\n",
        "\n",
        "val_targets = np.array([1 if np.sum(s['targets']) > 0.5 else 0 for s in val_samples_final])\n",
        "val_pos_idxs = np.where(val_targets == 1)[0]\n",
        "val_neg_idxs = np.where(val_targets == 0)[0]\n",
        "num_neg_val = min(len(val_neg_idxs), len(val_pos_idxs) * 3)\n",
        "val_balanced_idxs = np.concatenate([val_pos_idxs, np.random.choice(val_neg_idxs, num_neg_val, replace=False)])\n",
        "val_sampler = SubsetRandomSampler(val_balanced_idxs)\n",
        "\n",
        "BATCH_SIZE = 16\n",
        "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, sampler=train_sampler, num_workers=2, pin_memory=True, drop_last=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, sampler=val_sampler, num_workers=2, pin_memory=True)\n",
        "\n",
        "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "path_best_f1 = f\"nfl_r2plus1d_{TARGET_VIEW}_BEST_F1.pth\"\n",
        "path_best_loss = f\"nfl_r2plus1d_{TARGET_VIEW}_BEST_LOSS.pth\"\n",
        "\n",
        "pos_weight_val = 3.0\n",
        "criterion = nn.BCEWithLogitsLoss(pos_weight=torch.tensor([pos_weight_val]).to(DEVICE))\n",
        "\n",
        "model = NFLImpactModel(num_frames=16, hidden_dim=256, dropout_prob=0.5).to(DEVICE)\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=1.5e-5, weight_decay=1e-2)\n",
        "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='max', factor=0.5, patience=3)\n",
        "scaler = torch.amp.GradScaler('cuda')\n",
        "\n",
        "\n",
        "NUM_EPOCHS = 20\n",
        "best_val_f1 = 0.0\n",
        "best_val_loss = float('inf')\n",
        "\n",
        "print(f\"\\n START TRAINING | View: {TARGET_VIEW} | Loss Weight: {pos_weight_val}x\")\n",
        "\n",
        "print(f\" Validation Set: {len(val_pos_idxs)} Positives vs {num_neg_val} Negatives (Ratio 1:3)\")\n",
        "\n",
        "for epoch in range(NUM_EPOCHS):\n",
        "    # --- TRAIN ---\n",
        "    model.train()\n",
        "    train_loss = 0.0\n",
        "    all_train_preds, all_train_targets = [], []\n",
        "\n",
        "    pbar = tqdm(train_loader, desc=f\"Ep {epoch+1}/{NUM_EPOCHS} [Train]\")\n",
        "    for inputs, targets in pbar:\n",
        "        inputs, targets = inputs.to(DEVICE), targets.to(DEVICE)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        with torch.amp.autocast('cuda'):\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, targets)\n",
        "\n",
        "        scaler.scale(loss).backward()\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
        "        scaler.step(optimizer)\n",
        "        scaler.update()\n",
        "\n",
        "        train_loss += loss.item()\n",
        "        preds_bin = (torch.sigmoid(outputs).max(dim=1)[0] > 0.5).detach().cpu().numpy()\n",
        "        targets_bin = (targets.max(dim=1)[0] > 0.5).detach().cpu().numpy()\n",
        "        all_train_preds.extend(preds_bin)\n",
        "        all_train_targets.extend(targets_bin)\n",
        "\n",
        "        pbar.set_postfix({'loss': f\"{loss.item():.4f}\"})\n",
        "\n",
        "    avg_train_loss = train_loss / len(train_loader)\n",
        "    train_f1 = f1_score(all_train_targets, all_train_preds, zero_division=0)\n",
        "\n",
        "\n",
        "    # --- VALIDATION ---\n",
        "    model.eval()\n",
        "    val_loss = 0.0\n",
        "    all_val_preds = []\n",
        "    all_val_targets = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for inputs, targets in val_loader:\n",
        "            inputs, targets = inputs.to(DEVICE), targets.to(DEVICE)\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, targets)\n",
        "            val_loss += loss.item()\n",
        "\n",
        "            preds_prob = torch.sigmoid(outputs)\n",
        "            preds_bin = (preds_prob.max(dim=1)[0] > 0.45).cpu().numpy().astype(int)\n",
        "            targets_bin = (targets.max(dim=1)[0] > 0.5).cpu().numpy().astype(int)\n",
        "\n",
        "            all_val_preds.extend(preds_bin)\n",
        "            all_val_targets.extend(targets_bin)\n",
        "\n",
        "    avg_val_loss = val_loss / len(val_loader) if len(val_loader) > 0 else 0.0\n",
        "\n",
        "    val_prec = precision_score(all_val_targets, all_val_preds, zero_division=0)\n",
        "    val_rec = recall_score(all_val_targets, all_val_preds, zero_division=0)\n",
        "    val_f1 = f1_score(all_val_targets, all_val_preds, zero_division=0)\n",
        "\n",
        "    print(f\"REPORT EPOCH {epoch+1}\")\n",
        "    print(f\"   Train -> Loss: {avg_train_loss:.4f} | F1: {train_f1:.4f}\")\n",
        "    print(f\"   Val   -> Loss: {avg_val_loss:.4f} | F1: {val_f1:.4f} | Prec: {val_prec:.4f} | Rec: {val_rec:.4f}\")\n",
        "\n",
        "    scheduler.step(val_f1)\n",
        "\n",
        "    # --- SALVATAGGIO ---\n",
        "    if val_f1 > best_val_f1:\n",
        "        print(f\"   NEW BEST F1! ({best_val_f1:.4f} -> {val_f1:.4f}) Saving...\")\n",
        "        best_val_f1 = val_f1\n",
        "        torch.save(model.state_dict(), path_best_f1)\n",
        "\n",
        "    if avg_val_loss < best_val_loss:\n",
        "        best_val_loss = avg_val_loss\n",
        "        torch.save(model.state_dict(), path_best_loss)\n",
        "        print(f\"   New Best Loss: {avg_val_loss:.4f}\")\n",
        "\n",
        "print(f\"\\nTraining complete.\")\n",
        "print(f\"Model best F1: {path_best_f1}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "h2vy6dk4OuWU",
        "outputId": "4a7d31c3-e5d2-48bd-aa09-28c24c09a740"
      },
      "outputs": [
        {
          "data": {
            "application/javascript": "\n    async function download(id, filename, size) {\n      if (!google.colab.kernel.accessAllowed) {\n        return;\n      }\n      const div = document.createElement('div');\n      const label = document.createElement('label');\n      label.textContent = `Downloading \"${filename}\": `;\n      div.appendChild(label);\n      const progress = document.createElement('progress');\n      progress.max = size;\n      div.appendChild(progress);\n      document.body.appendChild(div);\n\n      const buffers = [];\n      let downloaded = 0;\n\n      const channel = await google.colab.kernel.comms.open(id);\n      // Send a message to notify the kernel that we're ready.\n      channel.send({})\n\n      for await (const message of channel.messages) {\n        // Send a message to notify the kernel that we're ready.\n        channel.send({})\n        if (message.buffers) {\n          for (const buffer of message.buffers) {\n            buffers.push(buffer);\n            downloaded += buffer.byteLength;\n            progress.value = downloaded;\n          }\n        }\n      }\n      const blob = new Blob(buffers, {type: 'application/binary'});\n      const a = document.createElement('a');\n      a.href = window.URL.createObjectURL(blob);\n      a.download = filename;\n      div.appendChild(a);\n      a.click();\n      div.remove();\n    }\n  ",
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/javascript": "download(\"download_0fdacee8-9fbc-42e8-84d5-9887102e470c\", \"nfl_r2plus1d_Endzone_BEST_F1.pth\", 125935714)",
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "files.download(path_best_f1)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}

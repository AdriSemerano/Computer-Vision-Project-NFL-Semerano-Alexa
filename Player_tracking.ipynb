{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 354182,
     "status": "ok",
     "timestamp": 1767970761289,
     "user": {
      "displayName": "adriano semerano",
      "userId": "07153423183131377818"
     },
     "user_tz": -60
    },
    "id": "_R6Uyh4K-7sW",
    "outputId": "b94a4915-9127-485f-eb35-eabb603d80f6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: gdown in /usr/local/lib/python3.12/dist-packages (5.2.0)\n",
      "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.12/dist-packages (from gdown) (4.13.5)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from gdown) (3.20.0)\n",
      "Requirement already satisfied: requests[socks] in /usr/local/lib/python3.12/dist-packages (from gdown) (2.32.4)\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from gdown) (4.67.1)\n",
      "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.12/dist-packages (from beautifulsoup4->gdown) (2.8)\n",
      "Requirement already satisfied: typing-extensions>=4.0.0 in /usr/local/lib/python3.12/dist-packages (from beautifulsoup4->gdown) (4.15.0)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests[socks]->gdown) (3.4.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests[socks]->gdown) (3.11)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests[socks]->gdown) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests[socks]->gdown) (2025.11.12)\n",
      "Requirement already satisfied: PySocks!=1.5.7,>=1.5.6 in /usr/local/lib/python3.12/dist-packages (from requests[socks]->gdown) (1.7.1)\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (4.67.1)\n",
      "Requirement already satisfied: ultralytics in /usr/local/lib/python3.12/dist-packages (8.3.250)\n",
      "Requirement already satisfied: numpy>=1.23.0 in /usr/local/lib/python3.12/dist-packages (from ultralytics) (2.0.2)\n",
      "Requirement already satisfied: matplotlib>=3.3.0 in /usr/local/lib/python3.12/dist-packages (from ultralytics) (3.10.0)\n",
      "Requirement already satisfied: opencv-python>=4.6.0 in /usr/local/lib/python3.12/dist-packages (from ultralytics) (4.12.0.88)\n",
      "Requirement already satisfied: pillow>=7.1.2 in /usr/local/lib/python3.12/dist-packages (from ultralytics) (11.3.0)\n",
      "Requirement already satisfied: pyyaml>=5.3.1 in /usr/local/lib/python3.12/dist-packages (from ultralytics) (6.0.3)\n",
      "Requirement already satisfied: requests>=2.23.0 in /usr/local/lib/python3.12/dist-packages (from ultralytics) (2.32.4)\n",
      "Requirement already satisfied: scipy>=1.4.1 in /usr/local/lib/python3.12/dist-packages (from ultralytics) (1.16.3)\n",
      "Requirement already satisfied: torch>=1.8.0 in /usr/local/lib/python3.12/dist-packages (from ultralytics) (2.9.0+cu126)\n",
      "Requirement already satisfied: torchvision>=0.9.0 in /usr/local/lib/python3.12/dist-packages (from ultralytics) (0.24.0+cu126)\n",
      "Requirement already satisfied: psutil>=5.8.0 in /usr/local/lib/python3.12/dist-packages (from ultralytics) (5.9.5)\n",
      "Requirement already satisfied: polars>=0.20.0 in /usr/local/lib/python3.12/dist-packages (from ultralytics) (1.31.0)\n",
      "Requirement already satisfied: ultralytics-thop>=2.0.18 in /usr/local/lib/python3.12/dist-packages (from ultralytics) (2.0.18)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.3.0->ultralytics) (1.3.3)\n",
      "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.3.0->ultralytics) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.3.0->ultralytics) (4.61.1)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.3.0->ultralytics) (1.4.9)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.3.0->ultralytics) (25.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.3.0->ultralytics) (3.2.5)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.3.0->ultralytics) (2.9.0.post0)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests>=2.23.0->ultralytics) (3.4.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests>=2.23.0->ultralytics) (3.11)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests>=2.23.0->ultralytics) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests>=2.23.0->ultralytics) (2025.11.12)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (3.20.0)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (4.15.0)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (75.2.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (1.14.0)\n",
      "Requirement already satisfied: networkx>=2.5.1 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (3.6.1)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (3.1.6)\n",
      "Requirement already satisfied: fsspec>=0.8.5 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (2025.3.0)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (12.6.77)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (12.6.77)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (12.6.80)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (9.10.2.21)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (12.6.4.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (11.3.0.4)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (10.3.7.77)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (11.7.1.2)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (12.5.4.2)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (0.7.1)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.27.5 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (2.27.5)\n",
      "Requirement already satisfied: nvidia-nvshmem-cu12==3.3.20 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (3.3.20)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (12.6.77)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (12.6.85)\n",
      "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (1.11.1.6)\n",
      "Requirement already satisfied: triton==3.5.0 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (3.5.0)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.7->matplotlib>=3.3.0->ultralytics) (1.17.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch>=1.8.0->ultralytics) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch>=1.8.0->ultralytics) (3.0.3)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['/content/train/reid_checkpoints/phase1_reid_hybrid_ENDZONE_epoch_20.pth',\n",
       " '/content/train/reid_checkpoints/phase1_reid_hybrid_SIDELINE_epoch_20.pth',\n",
       " '/content/train/reid_checkpoints/reid_hybrid_ENDZONE_best.pth',\n",
       " '/content/train/reid_checkpoints/reid_hybrid_SIDELINE_best.pth',\n",
       " '/content/train/best_dualview_96.pth',\n",
       " '/content/train/best_siamese_matcher_endzone.pth',\n",
       " '/content/train/best_siamese_matcher.pth',\n",
       " '/content/train/best_weights_players_detection.pt',\n",
       " '/content/train/best.pt',\n",
       " '/content/train/ckpt.t7',\n",
       " '/content/train/csv_helmets.zip',\n",
       " '/content/train/embeddings.zip',\n",
       " '/content/train/helmet_dataset.yaml',\n",
       " '/content/train/helmets_context_extracted.zip',\n",
       " '/content/train/helmets_extracted.zip',\n",
       " '/content/train/reid_gt_metadata_ENDZONE.csv',\n",
       " '/content/train/reid_gt_metadata_SIDELINE.csv',\n",
       " '/content/train/reid_gt_metadata.csv',\n",
       " '/content/train/train_player_tracking_frames.csv',\n",
       " '/content/train/video_directions.csv',\n",
       " '/content/train/Videos_dataset.zip']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "!pip install gdown\n",
    "!pip install tqdm\n",
    "!pip install ultralytics\n",
    "\n",
    "\n",
    "\n",
    "import gdown          \n",
    "import os\n",
    "import shutil\n",
    "import zipfile\n",
    "from google.colab import files \n",
    "import random\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from collections import defaultdict\n",
    "from collections import deque\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.models as models\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "import cv2\n",
    "from tqdm import tqdm \n",
    "from PIL import Image\n",
    "from IPython.display import display, FileLink, Markdown\n",
    "from collections import defaultdict\n",
    "from sklearn.metrics import confusion_matrix, precision_score, recall_score, f1_score\n",
    "from scipy.optimize import linear_sum_assignment\n",
    "\n",
    "url_dataset_1 = \"https://drive.google.com/drive/folders/1nO_javaDEk0yFcglOX7WoqRNj_MtCEDd?usp=sharing\"\n",
    "url_dataset_2 = \"https://drive.google.com/drive/folders/1VW5Y9-iUPNtz46uPFAVW__lu4WtJ1gAi?usp=sharing\"\n",
    "url_dataset_3 = \"https://drive.google.com/drive/folders/1uwbs9kUTnRK9bE-B4QLdnEMVp_lgPRxM?usp=sharing\"\n",
    "gdown.download_folder(url_dataset_1, quiet=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 328099,
     "status": "ok",
     "timestamp": 1767971089379,
     "user": {
      "displayName": "adriano semerano",
      "userId": "07153423183131377818"
     },
     "user_tz": -60
    },
    "id": "YgcbfOBmJvJf",
    "outputId": "add22eaa-cd04-4ae0-cd20-bbcd5d3a7a57"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Cartella 'train' spostata in 'datasets/Dataset'\n",
      "File zip datasets/Dataset/Videos_dataset.zip estratto e rimosso.\n",
      "File zip datasets/Dataset/csv_helmets.zip estratto e rimosso.\n",
      "File zip datasets/Dataset/embeddings.zip estratto e rimosso.\n",
      "File zip datasets/Dataset/helmets_context_extracted.zip estratto e rimosso.\n",
      "File zip datasets/Dataset/helmets_extracted.zip estratto e rimosso.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "original_path = \"train\"\n",
    "new_base = \"datasets\"\n",
    "new_path = os.path.join(new_base, \"Dataset\")\n",
    "\n",
    "os.makedirs(new_base, exist_ok=True)\n",
    "\n",
    "if not os.path.exists(new_path):\n",
    "    shutil.move(original_path, new_path)\n",
    "    print(f\" Folder '{original_path}' moved in '{new_path}'\")\n",
    "dataset_path = \"datasets/Dataset\"\n",
    "\n",
    "# extract zip files and remove them\n",
    "def extract_and_remove_zip(zip_path, extract_to):\n",
    "    with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
    "        zip_ref.extractall(extract_to)\n",
    "    os.remove(zip_path)\n",
    "    print(f\"File zip {zip_path} extracted and removed.\")\n",
    "\n",
    "zip_files = [f for f in os.listdir(dataset_path) if f.endswith('.zip')]\n",
    "for zip_file in zip_files:\n",
    "    zip_path = os.path.join(dataset_path, zip_file)\n",
    "    extract_and_remove_zip(zip_path, dataset_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iYPYbBaU0_6f"
   },
   "outputs": [],
   "source": [
    "\n",
    "ROOT_DIR = \"/content/datasets/Dataset\"\n",
    "CHECKPOINT_DIR = os.path.join(ROOT_DIR, \"reid_checkpoints\")\n",
    "\n",
    "CROPS_ROOT = os.path.join(ROOT_DIR, \"helmets_extracted\")\n",
    "\n",
    "METADATA_PATH_BASE = os.path.join(ROOT_DIR, \"reid_gt_metadata_\")\n",
    "\n",
    "\n",
    "OUTPUT_VIDEOS_DIR = \"/content/tracked_output_samples\"\n",
    "os.makedirs(OUTPUT_VIDEOS_DIR, exist_ok=True)\n",
    "VIDEOS_ROOT = os.path.join(ROOT_DIR, \"Videos_dataset\", \"train\")\n",
    "\n",
    "FRAME_W = 1280\n",
    "FRAME_H = 720\n",
    "VIDEO_FPS = 59.94\n",
    "EMBEDDING_DIM = 128\n",
    "POSITIONAL_FEATURES_DIM = 4\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "MAX_COSINE_DISTANCE = 0.50\n",
    "MAX_IOU_DISTANCE = 0.75\n",
    "MAX_AGE = 60\n",
    "N_INIT = 3\n",
    "\n",
    "MAX_CENTER_DISTANCE_REACQ = 200 \n",
    "MAX_COSINE_DISTANCE_REACQ = 0.60 \n",
    "WEIGHT_REID_REACQ = 0.8  \n",
    "WEIGHT_DIST_REACQ = 0.2 \n",
    "\n",
    "N_SAMPLES_VIDEOS = 3 \n",
    "MAX_SECONDS_PER_VIDEO = None \n",
    "FONT_SCALE = 0.6\n",
    "THICKNESS = 2\n",
    "TEXT_COLOR = (255, 255, 255) \n",
    "\n",
    "\n",
    "EXTRACTORS = {}\n",
    "METADATA_GT = {}\n",
    "PROCESSED_VIDEOS = defaultdict(list)\n",
    "TRACKING_RESULTS = defaultdict(dict)\n",
    "TRACK_EMBEDDINGS_CACHE = defaultdict(lambda: defaultdict(dict))\n",
    "\n",
    "def get_metadata(view_type):\n",
    "    global METADATA_GT, METADATA_PATH_BASE, CROPS_ROOT\n",
    "    if view_type in METADATA_GT:\n",
    "        return METADATA_GT[view_type]\n",
    "\n",
    "    view = view_type.upper()\n",
    "\n",
    "    if view == 'ENDZONE':\n",
    "        metadata_path = os.path.join(ROOT_DIR, \"reid_gt_metadata_ENDZONE.csv\")\n",
    "    elif view == 'SIDELINE':\n",
    "        metadata_path = os.path.join(ROOT_DIR, \"reid_gt_metadata_SIDELINE.csv\")\n",
    "    else:\n",
    "        raise FileNotFoundError(f\"CSV not found for {view}.\")\n",
    "\n",
    "    if not os.path.exists(metadata_path):\n",
    "        raise FileNotFoundError(f\"Metadata GT not founf for {view}: {metadata_path}\")\n",
    "\n",
    "    df = pd.read_csv(metadata_path)\n",
    "\n",
    "    df['video_folder'] = df['video_file'].apply(lambda x: os.path.splitext(x)[0])\n",
    "\n",
    "   \n",
    "    df['crop_path'] = df['image_path'].apply(lambda rel_path: os.path.join(CROPS_ROOT, rel_path))\n",
    "\n",
    "    METADATA_GT[view_type] = df\n",
    "    return df\n",
    "\n",
    "\n",
    "\n",
    "def run_tracking_logic(video_folder_name, metadata_df, max_frames_limit=None):\n",
    "    global N_INIT\n",
    "    global TRACK_EMBEDDINGS_CACHE \n",
    "\n",
    "    TRACK_EMBEDDINGS_CACHE[video_folder_name] = defaultdict(dict)\n",
    "\n",
    "    if 'Endzone' in video_folder_name:\n",
    "        view_type = 'ENDZONE'\n",
    "    elif 'Sideline' in video_folder_name:\n",
    "        view_type = 'SIDELINE'\n",
    "    else:\n",
    "        print(f\"LOG: Ignoring {video_folder_name}. View label not existing.\")\n",
    "        return []\n",
    "\n",
    "\n",
    "    video_metadata = metadata_df[metadata_df['video_folder'] == video_folder_name].copy().reset_index(drop=True)\n",
    "    if video_metadata.empty:\n",
    "        print(f\" LOG: metadata not found for {video_folder_name}.\")\n",
    "        return []\n",
    "\n",
    "    print(f\"\\nTracking: {video_folder_name} ({view_type})\")\n",
    "    all_frame_nums = sorted(video_metadata['frame_num'].unique())\n",
    "    total_frames = len(all_frame_nums)\n",
    "\n",
    "    if max_frames_limit and max_frames_limit < total_frames:\n",
    "        print(f\"⚠️ LOG: Limiting process to {max_frames_limit} frames on {total_frames}.\")\n",
    "        all_frame_nums = all_frame_nums[:max_frames_limit]\n",
    "        total_frames = len(all_frame_nums)\n",
    "    else:\n",
    "        print(f\"ℹ️ LOG: processed {total_frames}.\")\n",
    "\n",
    "    try:\n",
    "        extractor = get_feature_extractor(view_type)\n",
    "    except FileNotFoundError as e:\n",
    "        print(f\"❌ ERROR: {e}\")\n",
    "        return []\n",
    "\n",
    "    all_tracks = []\n",
    "    video_tracking_records = []\n",
    "\n",
    "\n",
    "    with tqdm(total=total_frames, desc=f\"Tracking {view_type}: {video_folder_name}\") as pbar:\n",
    "        for frame_num in all_frame_nums:\n",
    "\n",
    "            current_frame_data = video_metadata[video_metadata['frame_num'] == frame_num].copy()\n",
    "            new_detections_data = []\n",
    "\n",
    "            for _, row in current_frame_data.iterrows():\n",
    "                try:\n",
    "                    crop = cv2.imread(row['crop_path'])\n",
    "                    if crop is None or crop.size == 0:\n",
    "                        continue\n",
    "                except Exception:\n",
    "                    continue\n",
    "\n",
    "                x1, y1, x2, y2 = int(row['x1']), int(row['y1']), int(row['x2']), int(row['y2'])\n",
    "                w = x2 - x1\n",
    "                h = y2 - y1\n",
    "                x_center = x1 + w / 2\n",
    "                y_center = y1 + h / 2\n",
    "\n",
    "                bbox_tlwh = np.array([x1, y1, w, h], dtype=np.float32)\n",
    "                bbox_abs = np.array([x_center, y_center, w, h], dtype=np.float32)\n",
    "\n",
    "                new_detections_data.append((bbox_tlwh, crop, bbox_abs))\n",
    "\n",
    "            num_detections = len(new_detections_data)\n",
    "            num_active_tracks_before = len(all_tracks)\n",
    "\n",
    "            if num_detections == 0 and num_active_tracks_before == 0:\n",
    "                pbar.update(1)\n",
    "                continue\n",
    "\n",
    "            matches, all_tracks_before_cleanup, unmatched_det_indices = associate_detections_to_tracks(\n",
    "                all_tracks, new_detections_data, extractor)\n",
    "\n",
    "            matched_tracks = set()\n",
    "\n",
    "            for det_idx, track in matches:\n",
    "                _, crop, bbox_abs = new_detections_data[det_idx]\n",
    "\n",
    "                current_embedding = extractor([crop], np.array([bbox_abs]))[0]\n",
    "\n",
    "                track.update(current_embedding, bbox_abs)\n",
    "                matched_tracks.add(track)\n",
    "\n",
    "                TRACK_EMBEDDINGS_CACHE[video_folder_name][frame_num][track.track_id] = current_embedding\n",
    "\n",
    "            missed_tracks = 0\n",
    "            deleted_tracks = 0\n",
    "            tracks_to_keep = []\n",
    "\n",
    "            for track in all_tracks:\n",
    "                if track not in matched_tracks:\n",
    "                    track.mark_missed()\n",
    "                    missed_tracks += 1\n",
    "\n",
    "                if track.is_deleted():\n",
    "                    deleted_tracks += 1\n",
    "                else:\n",
    "                    tracks_to_keep.append(track)\n",
    "\n",
    "            all_tracks = tracks_to_keep\n",
    "\n",
    "\n",
    "            newly_created_tracks = []\n",
    "            for det_idx in unmatched_det_indices:\n",
    "                _, crop, bbox_abs = new_detections_data[det_idx]\n",
    "                new_embedding = extractor([crop], np.array([bbox_abs]))[0]\n",
    "                new_track = SimpleTrack(new_embedding, bbox_abs, frame_num)\n",
    "                all_tracks.append(new_track)\n",
    "                newly_created_tracks.append(new_track.track_id)\n",
    "\n",
    "                TRACK_EMBEDDINGS_CACHE[video_folder_name][frame_num][new_track.track_id] = new_embedding\n",
    "\n",
    "\n",
    "            for track in all_tracks:\n",
    "\n",
    "                if track.time_since_update == 0:\n",
    "                    x1, y1, x2, y2 = track.predicted_bbox_tlbr.tolist()\n",
    "\n",
    "                    video_tracking_records.append({\n",
    "                        'frame_num': frame_num,\n",
    "                        'track_id': track.track_id,\n",
    "                        'x_min': x1,\n",
    "                        'y_min': y1,\n",
    "                        'x_max': x2,\n",
    "                        'y_max': y2,\n",
    "                    })\n",
    "\n",
    "                elif track.is_missing_but_active():\n",
    "                    continue\n",
    "\n",
    "                else:\n",
    "                    continue \n",
    "\n",
    "\n",
    "            num_confirmed_tracks_after = len([t for t in all_tracks if t.is_confirmed()])\n",
    "\n",
    "            pbar.update(1)\n",
    "\n",
    "    num_total_tracks = SimpleTrack._count\n",
    "    print(f\"\\n Tracking complete: {video_folder_name}\")\n",
    "    print(f\"  -> # ID Tracks Generated: {num_total_tracks}\")\n",
    "    print(f\"  -> Cache Embedding: {sum(len(v) for f in TRACK_EMBEDDINGS_CACHE[video_folder_name].values() for v in f.values())} embedding saved.\")\n",
    "\n",
    "    return video_tracking_records\n",
    "\n",
    "\n",
    "\n",
    "class SimpleTrack:\n",
    "    _count = 0\n",
    "\n",
    "    def __init__(self, embedding, bbox_abs, frame_num):\n",
    "        SimpleTrack._count += 1\n",
    "        self.track_id = SimpleTrack._count\n",
    "\n",
    "        self.last_bbox_abs = bbox_abs.copy()\n",
    "\n",
    "        self.embeddings = deque([embedding], maxlen=10)\n",
    "        self.hits = 1\n",
    "        self.time_since_update = 0\n",
    "        self.start_frame = frame_num\n",
    "        self.deleted = False\n",
    "\n",
    "    def update(self, embedding, bbox_abs):\n",
    "        self.embeddings.append(embedding)\n",
    "        self.last_bbox_abs = bbox_abs.copy()\n",
    "        self.hits += 1\n",
    "        self.time_since_update = 0\n",
    "\n",
    "    def mark_missed(self):\n",
    "        global MAX_AGE\n",
    "        self.time_since_update += 1\n",
    "\n",
    "        if self.time_since_update > MAX_AGE:\n",
    "            self.deleted = True\n",
    "\n",
    "    def is_missing_but_active(self):\n",
    "        return self.time_since_update > 0 and not self.is_deleted()\n",
    "\n",
    "    def is_confirmed(self):\n",
    "        return self.hits >= N_INIT\n",
    "\n",
    "    def is_deleted(self):\n",
    "        return self.deleted\n",
    "\n",
    "    @property\n",
    "    def predicted_bbox_tlbr(self):\n",
    "        x_c, y_c, w, h = self.last_bbox_abs\n",
    "        return np.array([x_c - w/2, y_c - h/2, x_c + w/2, y_c + h/2])\n",
    "\n",
    "    @property\n",
    "    def mean_embedding(self):\n",
    "        mean_emb = np.mean(self.embeddings, axis=0)\n",
    "        norm = np.linalg.norm(mean_emb)\n",
    "        if norm > 0:\n",
    "            return (mean_emb / norm).reshape(1, -1)\n",
    "        else:\n",
    "            return mean_emb.reshape(1, -1)\n",
    "\n",
    "\n",
    "def bbox_iou_simple(boxA, boxB):\n",
    "    \n",
    "    xA = max(boxA[0], boxB[0])\n",
    "    yA = max(boxA[1], boxB[1])\n",
    "    xB = min(boxA[2], boxB[2])\n",
    "    yB = min(boxA[3], boxB[3])\n",
    "\n",
    "    interArea = max(0, xB - xA) * max(0, yB - yA)\n",
    "\n",
    "    boxAArea = (boxA[2] - boxA[0]) * (boxA[3] - boxA[1])\n",
    "    boxBArea = (boxB[2] - boxB[0]) * (boxB[3] - boxB[1])\n",
    "\n",
    "    union = float(boxAArea + boxBArea - interArea)\n",
    "    if union == 0:\n",
    "        return 0.0\n",
    "\n",
    "    iou = interArea / union\n",
    "    return iou\n",
    "\n",
    "\n",
    "class HybridFrameReIDModel(nn.Module):\n",
    "    def __init__(self, embedding_dim=128, positional_features_dim=4):\n",
    "        super().__init__()\n",
    "        resnet = models.resnet18(weights=models.ResNet18_Weights.IMAGENET1K_V1)\n",
    "        self.features = nn.Sequential(*(list(resnet.children())[:-2]))\n",
    "        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n",
    "\n",
    "        VISUAL_FEATURE_DIM = 512\n",
    "        COMBINED_DIM = VISUAL_FEATURE_DIM + positional_features_dim\n",
    "        self.embedding_layer = nn.Linear(COMBINED_DIM, embedding_dim)\n",
    "\n",
    "    def forward(self, x_image, x_positional):\n",
    "        x_image = self.features(x_image)\n",
    "        x_image = self.avgpool(x_image)\n",
    "        x_image = torch.flatten(x_image, 1)\n",
    "        combined_features = torch.cat((x_image, x_positional), dim=1)\n",
    "        embedding = self.embedding_layer(combined_features)\n",
    "        return nn.functional.normalize(embedding, p=2, dim=1)\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToPILImage(),\n",
    "    transforms.Resize((64, 64)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "class CustomFeatureExtractor:\n",
    "    def __init__(self, reid_model, transform, device, frame_w, frame_h):\n",
    "        self.reid_model = reid_model\n",
    "        self.transform = transform\n",
    "        self.device = device\n",
    "        self.FRAME_W = frame_w\n",
    "        self.FRAME_H = frame_h\n",
    "        self.reid_model.eval()\n",
    "\n",
    "    def __call__(self, img_crops, bbox_xywh_abs):\n",
    "        if len(img_crops) == 0:\n",
    "            return np.empty((0, self.reid_model.embedding_layer.out_features), dtype=np.float32)\n",
    "\n",
    "        image_tensors = []\n",
    "        for crop in img_crops:\n",
    "            if crop is not None and crop.size > 0:\n",
    "                rgb_crop = cv2.cvtColor(crop, cv2.COLOR_BGR2RGB)\n",
    "                image_tensors.append(self.transform(rgb_crop))\n",
    "            else:\n",
    "                image_tensors.append(torch.zeros(3, 64, 64, dtype=torch.float32))\n",
    "\n",
    "        image_batch = torch.stack(image_tensors).to(self.device)\n",
    "\n",
    "        x_c, y_c, w_bb, h_bb = bbox_xywh_abs[:, 0], bbox_xywh_abs[:, 1], bbox_xywh_abs[:, 2], bbox_xywh_abs[:, 3]\n",
    "        x_center_norm = torch.tensor(x_c / self.FRAME_W, dtype=torch.float32)\n",
    "        y_center_norm = torch.tensor(y_c / self.FRAME_H, dtype=torch.float32)\n",
    "        w_bb_norm = torch.tensor(w_bb / self.FRAME_W, dtype=torch.float32)\n",
    "        h_bb_norm = torch.tensor(h_bb / self.FRAME_H, dtype=torch.float32)\n",
    "\n",
    "        positional_data_batch = torch.stack([x_center_norm, y_center_norm, w_bb_norm, h_bb_norm], dim=1).to(self.device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            embeddings = self.reid_model(image_batch, positional_data_batch)\n",
    "\n",
    "        return embeddings.cpu().numpy()\n",
    "\n",
    "\n",
    "def visualize_and_save_video(video_folder_name, fps, tracking_data, data_key, suffix):\n",
    "    global VIDEOS_ROOT, OUTPUT_VIDEOS_DIR, THICKNESS, FONT_SCALE, TEXT_COLOR\n",
    "\n",
    "    print(f\"\\n Visualization: {video_folder_name} ({suffix.strip('_')})\")\n",
    "\n",
    "    if data_key not in tracking_data or not tracking_data[data_key]:\n",
    "        print(f\" Error: {video_folder_name}: Not found or empty '{data_key}'.\")\n",
    "        return\n",
    "\n",
    "    tracked_data = tracking_data[data_key] \n",
    "\n",
    "    video_file = video_folder_name + '.mp4'\n",
    "    VIDEO_PATH = os.path.join(VIDEOS_ROOT, video_file)\n",
    "\n",
    "    if not os.path.exists(VIDEO_PATH):\n",
    "        print(f\" Error: {VIDEO_PATH}.\")\n",
    "        return\n",
    "\n",
    "    cap = cv2.VideoCapture(VIDEO_PATH)\n",
    "    if not cap.isOpened():\n",
    "        print(f\" Error: {VIDEO_PATH}.\")\n",
    "        return\n",
    "\n",
    "    output_frames = []\n",
    "    current_frame_num = 0\n",
    "\n",
    "    ID_COLORS = {}\n",
    "    max_frame_to_read = max(tracked_data.keys()) if tracked_data else 0\n",
    "\n",
    "    if max_frame_to_read == 0:\n",
    "        print(f\" LOG: {video_folder_name}: No tracking.\")\n",
    "        cap.release()\n",
    "        return\n",
    "\n",
    "    num_frames_with_tracks = 0\n",
    "\n",
    "    with tqdm(desc=f\"Visualization {video_folder_name} {suffix}\", total=max_frame_to_read) as pbar:\n",
    "        while cap.isOpened() and current_frame_num < max_frame_to_read:\n",
    "            ret, frame = cap.read()\n",
    "            if not ret:\n",
    "                break\n",
    "\n",
    "            current_frame_num += 1\n",
    "\n",
    "            if current_frame_num in tracked_data:\n",
    "\n",
    "                frame_tracks = tracked_data[current_frame_num]\n",
    "\n",
    "                if frame_tracks:\n",
    "                    num_frames_with_tracks += 1\n",
    "\n",
    "                    if current_frame_num % 100 == 1:\n",
    "                        print(f\"      [F{current_frame_num}]: Drawing {len(frame_tracks)} tracks. E.G. BBox: {frame_tracks[0]['bbox']}\")\n",
    "\n",
    "                    for track_info in frame_tracks:\n",
    "                        track_id = track_info['id']\n",
    "                        x1, y1, x2, y2 = track_info['bbox']\n",
    "\n",
    "                        if track_id not in ID_COLORS:\n",
    "                            r = (track_id * 85) % 255\n",
    "                            g = (track_id * 15) % 255\n",
    "                            b = (track_id * 50) % 255\n",
    "                            ID_COLORS[track_id] = (int(b), int(g), int(r))\n",
    "\n",
    "                        color = ID_COLORS[track_id]\n",
    "\n",
    "                        cv2.rectangle(frame, (x1, y1), (x2, y2), color, THICKNESS)\n",
    "\n",
    "                        label = f\"ID: {track_id}\"\n",
    "                        (tw, th), _ = cv2.getTextSize(label, cv2.FONT_HERSHEY_SIMPLEX, FONT_SCALE, THICKNESS)\n",
    "\n",
    "                        cv2.rectangle(frame, (x1, max(0,y1-th-4)), (x1+tw+4, y1), (0,0,0), -1)\n",
    "                        cv2.putText(frame, label, (x1+2, y1-4), cv2.FONT_HERSHEY_SIMPLEX, FONT_SCALE, TEXT_COLOR, THICKNESS-1, cv2.LINE_AA)\n",
    "\n",
    "                output_frames.append(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB))\n",
    "            pbar.update(1)\n",
    "\n",
    "    cap.release()\n",
    "\n",
    "    if output_frames:\n",
    "        output_path_gif = os.path.join(OUTPUT_VIDEOS_DIR, f\"{video_folder_name}{suffix}.gif\") \n",
    "\n",
    "        first_frame = Image.fromarray(output_frames[0]).resize((640, 360))\n",
    "        pil_frames = [first_frame] + [Image.fromarray(f).resize((640, 360)) for f in output_frames[1:]]\n",
    "\n",
    "        pil_frames[0].save(\n",
    "            output_path_gif,\n",
    "            save_all=True,\n",
    "            append_images=pil_frames[1:],\n",
    "            duration=int(1000/fps),\n",
    "            loop=0\n",
    "        )\n",
    "        print(f\"\\n Visualization Completed: {video_folder_name}\")\n",
    "        print(f\"  -> Frame with tracks: {num_frames_with_tracks}\")\n",
    "        print(f\"  -> Saved example: {output_path_gif}\")\n",
    "\n",
    "    return True\n",
    "\n",
    "def get_feature_extractor(view_type):\n",
    "    global EXTRACTORS, CHECKPOINT_DIR\n",
    "    if view_type in EXTRACTORS:\n",
    "        return EXTRACTORS[view_type]\n",
    "\n",
    "    view = view_type.upper()\n",
    "\n",
    "    reid_model = HybridFrameReIDModel(EMBEDDING_DIM, POSITIONAL_FEATURES_DIM).to(device)\n",
    "\n",
    "    if view_type.lower() == 'sideline':\n",
    "        checkpoint_name = \"reid_hybrid_SIDELINE_best.pth\"\n",
    "    else:\n",
    "        checkpoint_name = \"reid_hybrid_ENDZONE_best.pth\"\n",
    "\n",
    "    checkpoint_path = os.path.join(CHECKPOINT_DIR, checkpoint_name)\n",
    "\n",
    "    if not os.path.exists(checkpoint_path):\n",
    "        raise FileNotFoundError(f\"Checkpoint not found for {view}: {checkpoint_path}\")\n",
    "\n",
    "    print(f\" Checkpoint weights: {checkpoint_path}\")\n",
    "    checkpoint = torch.load(checkpoint_path, map_location=device)\n",
    "\n",
    "    if isinstance(checkpoint, dict) and 'model_state_dict' in checkpoint:\n",
    "        reid_model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    else:\n",
    "        try:\n",
    "            reid_model.load_state_dict(checkpoint)\n",
    "        except RuntimeError as e:\n",
    "            print(f\" Error loading checkpoint: {e}\")\n",
    "            new_state_dict = {}\n",
    "            for k, v in checkpoint.items():\n",
    "                name = k.replace(\"module.\", \"\") if k.startswith(\"module.\") else k\n",
    "                new_state_dict[name] = v\n",
    "            reid_model.load_state_dict(new_state_dict)\n",
    "\n",
    "    reid_model.eval()\n",
    "\n",
    "    feature_extractor = CustomFeatureExtractor(reid_model, transform, device, FRAME_W, FRAME_H)\n",
    "\n",
    "    EXTRACTORS[view_type] = feature_extractor\n",
    "    print(f\"ℹ LOG: Re-ID {view} loaded from {checkpoint_name}.\")\n",
    "    return feature_extractor\n",
    "\n",
    "def associate_detections_to_tracks(all_tracks, new_detections_data, extractor):\n",
    "    global MAX_COSINE_DISTANCE, MAX_IOU_DISTANCE, MAX_CENTER_DISTANCE_REACQ\n",
    "    global WEIGHT_REID_REACQ, WEIGHT_DIST_REACQ, MAX_COSINE_DISTANCE_REACQ\n",
    "\n",
    "    WEIGHT_REID_PRIM = 0.7\n",
    "    WEIGHT_IOU_PRIM = 0.3\n",
    "    MAX_COMBINED_COST_PRIM = MAX_COSINE_DISTANCE * WEIGHT_REID_PRIM + (1.0 - MAX_IOU_DISTANCE) * WEIGHT_IOU_PRIM\n",
    "\n",
    "    active_tracks = [t for t in all_tracks if not t.is_deleted()]\n",
    "\n",
    "    if not active_tracks or not new_detections_data:\n",
    "        return [], active_tracks, list(range(len(new_detections_data)))\n",
    "\n",
    "    track_embeddings = np.vstack([t.mean_embedding for t in active_tracks])\n",
    "    track_bboxes_tlbr = np.array([t.predicted_bbox_tlbr for t in active_tracks]) \n",
    "\n",
    "    det_crops = [d[1] for d in new_detections_data]\n",
    "    det_bboxes_abs = np.vstack([d[2] for d in new_detections_data])\n",
    "    det_bboxes_tlbr = np.array([[d[0][0], d[0][1], d[0][0] + d[0][2], d[0][1] + d[0][3]] for d in new_detections_data])\n",
    "\n",
    "    det_embeddings = extractor(det_crops, det_bboxes_abs)\n",
    "\n",
    "    cosine_similarity_matrix = cosine_similarity(det_embeddings, track_embeddings)\n",
    "    identity_cost = 1.0 - cosine_similarity_matrix \n",
    "\n",
    "    iou_cost = np.zeros((len(new_detections_data), len(active_tracks)))\n",
    "    distance_cost = np.zeros((len(new_detections_data), len(active_tracks)))\n",
    "\n",
    "    det_centers = det_bboxes_abs[:, :2]\n",
    "    track_centers = np.array([t.last_bbox_abs[:2] for t in active_tracks])\n",
    "\n",
    "    IOU_GATE_THRESHOLD_PRIM = 0.25\n",
    "    GATE_VALUE = 100.0\n",
    "\n",
    "    for i in range(len(new_detections_data)):\n",
    "        for j, track in enumerate(active_tracks):\n",
    "            iou = bbox_iou_simple(det_bboxes_tlbr[i], track_bboxes_tlbr[j])\n",
    "\n",
    "            center_dist = np.linalg.norm(det_centers[i] - track_centers[j])\n",
    "\n",
    "            if iou < IOU_GATE_THRESHOLD_PRIM:\n",
    "                iou_cost[i, j] = GATE_VALUE\n",
    "            else:\n",
    "                iou_cost[i, j] = 1.0 - iou\n",
    "\n",
    "            if center_dist > MAX_CENTER_DISTANCE_REACQ:\n",
    "                distance_cost[i, j] = GATE_VALUE\n",
    "            else:\n",
    "                distance_cost[i, j] = center_dist / MAX_CENTER_DISTANCE_REACQ\n",
    "\n",
    "    COMBINED_COST_PRIM = identity_cost * WEIGHT_REID_PRIM + iou_cost * WEIGHT_IOU_PRIM\n",
    "\n",
    "    matches = []\n",
    "    unmatched_detections = list(range(len(new_detections_data)))\n",
    "    unmatched_tracks = list(range(len(active_tracks)))\n",
    "\n",
    "    cost_matrix_temp = COMBINED_COST_PRIM.copy()\n",
    "\n",
    "    while unmatched_detections and unmatched_tracks:\n",
    "        sub_matrix = cost_matrix_temp[np.ix_(unmatched_detections, unmatched_tracks)]\n",
    "        if sub_matrix.size == 0: break\n",
    "\n",
    "        min_cost_idx = np.unravel_index(sub_matrix.argmin(), sub_matrix.shape)\n",
    "        det_idx = unmatched_detections[min_cost_idx[0]]\n",
    "        track_idx = unmatched_tracks[min_cost_idx[1]]\n",
    "        cost = COMBINED_COST_PRIM[det_idx, track_idx]\n",
    "\n",
    "        if cost < MAX_COMBINED_COST_PRIM:\n",
    "            matches.append((det_idx, active_tracks[track_idx]))\n",
    "            unmatched_detections.remove(det_idx)\n",
    "            unmatched_tracks.remove(track_idx)\n",
    "        else:\n",
    "            break\n",
    "\n",
    "    missing_tracks_to_recover = [active_tracks[i] for i in unmatched_tracks if active_tracks[i].time_since_update > 0]\n",
    "    missing_track_indices = [i for i in unmatched_tracks if active_tracks[i].time_since_update > 0]\n",
    "\n",
    "    if not unmatched_detections or not missing_tracks_to_recover:\n",
    "        return matches, active_tracks, unmatched_detections\n",
    "\n",
    "    missing_identity_cost = identity_cost[np.ix_(unmatched_detections, missing_track_indices)]\n",
    "    missing_distance_cost = distance_cost[np.ix_(unmatched_detections, missing_track_indices)]\n",
    "\n",
    "\n",
    "    COMBINED_COST_REACQ = missing_identity_cost * WEIGHT_REID_REACQ + missing_distance_cost * WEIGHT_DIST_REACQ\n",
    "\n",
    "\n",
    "    REID_GATE_MASK = missing_identity_cost > MAX_COSINE_DISTANCE_REACQ\n",
    "\n",
    "    DISTANCE_GATE_MASK = missing_distance_cost >= GATE_VALUE\n",
    "\n",
    "\n",
    "    GATE_MASK = REID_GATE_MASK | DISTANCE_GATE_MASK\n",
    "\n",
    "    COMBINED_COST_REACQ[GATE_MASK] = GATE_VALUE\n",
    "\n",
    "\n",
    "    MAX_COMBINED_COST_REACQ = MAX_COSINE_DISTANCE_REACQ * WEIGHT_REID_REACQ + 1.0 * WEIGHT_DIST_REACQ\n",
    "\n",
    "    reacquisition_matches = []\n",
    "\n",
    "    while True:\n",
    "        if COMBINED_COST_REACQ.size == 0 or np.all(COMBINED_COST_REACQ >= GATE_VALUE):\n",
    "            break\n",
    "\n",
    "        min_cost_idx = np.unravel_index(COMBINED_COST_REACQ.argmin(), COMBINED_COST_REACQ.shape)\n",
    "\n",
    "        det_idx_in_unmatched = min_cost_idx[0]\n",
    "        track_idx_in_missing = min_cost_idx[1]\n",
    "\n",
    "        cost = COMBINED_COST_REACQ[det_idx_in_unmatched, track_idx_in_missing]\n",
    "\n",
    "        if cost < MAX_COMBINED_COST_REACQ:\n",
    "\n",
    "            original_det_idx = unmatched_detections[det_idx_in_unmatched]\n",
    "            track_to_update = missing_tracks_to_recover[track_idx_in_missing]\n",
    "\n",
    "            reacquisition_matches.append((original_det_idx, track_to_update))\n",
    "\n",
    "            COMBINED_COST_REACQ[det_idx_in_unmatched, :] = GATE_VALUE\n",
    "            COMBINED_COST_REACQ[:, track_idx_in_missing] = GATE_VALUE\n",
    "        else:\n",
    "            break\n",
    "\n",
    "    final_matches = matches + reacquisition_matches\n",
    "    final_unmatched_detections = [idx for idx in unmatched_detections if idx not in [m[0] for m in reacquisition_matches]]\n",
    "\n",
    "    return final_matches, active_tracks, final_unmatched_detections\n",
    "\n",
    "\n",
    "MIN_TRACK_LENGTH = 10         \n",
    "MAX_FRAME_GAP_MERGE = 15      \n",
    "MAX_COSINE_DISTANCE_PP = 0.55 \n",
    "\n",
    "\n",
    "MAX_DISPLACEMENT_ENDZONE = 113\n",
    "MAX_DISPLACEMENT_SIDELINE = 123 \n",
    "\n",
    "FRAME_WIDTH = 1280\n",
    "FRAME_HEIGHT = 720\n",
    "EDGE_TOLERANCE = 50\n",
    "\n",
    "\n",
    "def get_embedding_from_cache(video_id, frame_num, track_id):\n",
    "\n",
    "    global TRACK_EMBEDDINGS_CACHE\n",
    "    try:\n",
    "        return TRACK_EMBEDDINGS_CACHE[video_id][frame_num][track_id]\n",
    "    except KeyError:\n",
    "        return None\n",
    "\n",
    "def center_distance(boxA, boxB):\n",
    "    center_Ax = (boxA[0] + boxA[2]) / 2\n",
    "    center_Ay = (boxA[1] + boxA[3]) / 2\n",
    "    center_Bx = (boxB[0] + boxB[2]) / 2\n",
    "    center_By = (boxB[1] + boxB[3]) / 2\n",
    "    distance = np.sqrt((center_Ax - center_Bx)**2 + (center_Ay - center_By)**2)\n",
    "    return distance\n",
    "\n",
    "def is_at_edge(bbox_record):\n",
    "    \n",
    "    x_min = bbox_record['x_min']\n",
    "    x_max = bbox_record['x_max']\n",
    "    y_min = bbox_record['y_min']\n",
    "    y_max = bbox_record['y_max']\n",
    "\n",
    "    at_left = x_min <= EDGE_TOLERANCE\n",
    "    at_right = x_max >= FRAME_WIDTH - EDGE_TOLERANCE\n",
    "    at_top = y_min <= EDGE_TOLERANCE\n",
    "    at_bottom = y_max >= FRAME_HEIGHT - EDGE_TOLERANCE\n",
    "\n",
    "    return at_left or at_right or at_top or at_bottom\n",
    "\n",
    "\n",
    "def _apply_post_processing(df):\n",
    "    \n",
    "    if df.empty:\n",
    "        return df.copy()\n",
    "\n",
    "    df = df.copy().sort_values(by=['track_id', 'frame_num']).reset_index(drop=True)\n",
    "    video_id = df['video_id'].iloc[0]\n",
    "\n",
    "    if 'Endzone' in video_id:\n",
    "        max_displacement_per_frame = MAX_DISPLACEMENT_ENDZONE\n",
    "    elif 'Sideline' in video_id:\n",
    "        max_displacement_per_frame = MAX_DISPLACEMENT_SIDELINE\n",
    "    else:\n",
    "        max_displacement_per_frame = MAX_DISPLACEMENT_SIDELINE\n",
    "\n",
    "    print(f\"ℹ️ LOG: Soglia Max Spostamento per {video_id}: {max_displacement_per_frame} pixel/frame.\")\n",
    "\n",
    "    track_lengths = df.groupby('track_id')['frame_num'].nunique()\n",
    "    valid_track_ids = track_lengths[track_lengths >= MIN_TRACK_LENGTH].index\n",
    "    df_filtered = df[df['track_id'].isin(valid_track_ids)].copy()\n",
    "\n",
    "    if df_filtered.empty:\n",
    "        return pd.DataFrame()\n",
    "\n",
    "\n",
    "    grouped = df_filtered.groupby('track_id')\n",
    "    all_tracks_data = [track_df.sort_values(by='frame_num') for _, track_df in grouped]\n",
    "    all_tracks_data.sort(key=lambda x: x['frame_num'].iloc[0]) \n",
    "\n",
    "    merged_tracks = []\n",
    "    current_track_list = []\n",
    "    current_new_id = 1\n",
    "\n",
    "    for track_df_next in all_tracks_data:\n",
    "\n",
    "        if not current_track_list:\n",
    "            track_df_next['track_id'] = current_new_id\n",
    "            current_track_list = [track_df_next]\n",
    "            continue\n",
    "\n",
    "        last_track_df = current_track_list[-1]\n",
    "        last_frame = last_track_df['frame_num'].iloc[-1]\n",
    "        start_frame_next = track_df_next['frame_num'].iloc[0]\n",
    "        gap_size = start_frame_next - last_frame\n",
    "\n",
    "\n",
    "        if 0 < gap_size <= MAX_FRAME_GAP_MERGE:\n",
    "\n",
    "            last_record = last_track_df.iloc[-1]\n",
    "            next_record = track_df_next.iloc[0]\n",
    "\n",
    "\n",
    "            emb_last = get_embedding_from_cache(video_id, last_frame, last_record['track_id'])\n",
    "            emb_next = get_embedding_from_cache(video_id, start_frame_next, next_record['track_id'])\n",
    "\n",
    "            is_reid_match = False\n",
    "            if emb_last is not None and emb_next is not None:\n",
    "\n",
    "                similarity = cosine_similarity(emb_last.reshape(1, -1), emb_next.reshape(1, -1))[0][0]\n",
    "                cost_reid = 1.0 - similarity\n",
    "                is_reid_match = cost_reid <= MAX_COSINE_DISTANCE_PP\n",
    "\n",
    "            last_bbox = [\n",
    "                last_record['x_min'], last_record['y_min'],\n",
    "                last_record['x_max'], last_record['y_max']\n",
    "            ]\n",
    "            next_bbox = [\n",
    "                next_record['x_min'], next_record['y_min'],\n",
    "                next_record['x_max'], next_record['y_max']\n",
    "            ]\n",
    "\n",
    "            distance = center_distance(last_bbox, next_bbox)\n",
    "\n",
    "            max_allowed_distance = gap_size * max_displacement_per_frame\n",
    "            is_spatially_close = distance <= max_allowed_distance\n",
    "\n",
    "            is_edge_entry = is_at_edge(next_record)\n",
    "\n",
    "            if is_reid_match and is_spatially_close and (not is_edge_entry):\n",
    "                track_df_next['track_id'] = current_new_id \n",
    "                current_track_list.append(track_df_next)\n",
    "                continue\n",
    "\n",
    "\n",
    "\n",
    "        if current_track_list:\n",
    "            merged_df_segment = pd.concat(current_track_list, ignore_index=True)\n",
    "            merged_tracks.append(merged_df_segment)\n",
    "\n",
    "        current_new_id += 1\n",
    "        track_df_next['track_id'] = current_new_id\n",
    "        current_track_list = [track_df_next]\n",
    "\n",
    "    if current_track_list:\n",
    "        merged_df_segment = pd.concat(current_track_list, ignore_index=True)\n",
    "        merged_tracks.append(merged_df_segment)\n",
    "\n",
    "    if not merged_tracks:\n",
    "        return pd.DataFrame()\n",
    "\n",
    "    final_df = pd.concat(merged_tracks, ignore_index=True).sort_values(by=['track_id', 'frame_num']).reset_index(drop=True)\n",
    "\n",
    "    unique_ids = final_df['track_id'].unique()\n",
    "    id_map = {old_id: new_id + 1 for new_id, old_id in enumerate(unique_ids)}\n",
    "    final_df['track_id'] = final_df['track_id'].map(id_map)\n",
    "\n",
    "    print(f\" Post-Processing completed for {video_id}. Final Tracks: {len(unique_ids)}\")\n",
    "    return final_df\n",
    "\n",
    "def save_tracking_results_to_csv(df, video_id=\"consolidated\"):\n",
    "    output_csv_name = f\"{video_id}_tracking_log.csv\"\n",
    "    df.to_csv(output_csv_name, index=False, na_rep='NaN')\n",
    "\n",
    "    print(f\" LOG: File CSV saved: {output_csv_name}\")\n",
    "\n",
    "    print(\"\\n DOWNLOAD FILE:\")\n",
    "    try:\n",
    "        from google.colab import files\n",
    "        files.download(output_csv_name)\n",
    "    except Exception as e:\n",
    "        print(f\"Error: {e}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "background_save": true,
     "base_uri": "https://localhost:8080/"
    },
    "id": "4vKgP74-1BBC",
    "outputId": "aa81d3db-db3a-4cec-9abb-5d39f5d095f6"
   },
   "outputs": [],
   "source": [
    "TRACKING_RESULTS_VISUAL = {} \n",
    "\n",
    "\n",
    "all_video_folders = [d for d in os.listdir(CROPS_ROOT) if os.path.isdir(os.path.join(CROPS_ROOT, d))]\n",
    "endzone_folders = [f for f in all_video_folders if 'Endzone' in f]\n",
    "sideline_folders = [f for f in all_video_folders if 'Sideline' in f]\n",
    "\n",
    "\n",
    "videos_to_process_folders = endzone_folders + sideline_folders\n",
    "\n",
    "print(f\"Elaborating {len(videos_to_process_folders)}\")\n",
    "\n",
    "max_frames_limit = None\n",
    "try:\n",
    "    metadata_df_endzone = get_metadata('ENDZONE')\n",
    "    metadata_df_sideline = get_metadata('SIDELINE')\n",
    "except FileNotFoundError as e:\n",
    "    print(f\"ERROR: {e}\")\n",
    "    exit()\n",
    "all_tracking_records_post = []\n",
    "\n",
    "for video_folder in videos_to_process_folders:\n",
    "    metadata_df = metadata_df_endzone if 'Endzone' in video_folder else metadata_df_sideline\n",
    "\n",
    "    video_records_pre = run_tracking_logic(video_folder, metadata_df, max_frames_limit)\n",
    "    video_records_pre_df = pd.DataFrame(video_records_pre)\n",
    "    video_records_pre_df['video_id'] = video_folder \n",
    "\n",
    "    if video_records_pre_df.empty:\n",
    "        print(f\" LOG: No record for {video_folder}.\")\n",
    "        continue\n",
    "\n",
    "    video_records_post_df = _apply_post_processing(video_records_pre_df)\n",
    "\n",
    "    all_tracking_records_post.extend(video_records_post_df.to_dict('records'))\n",
    "\n",
    "    print(f\" LOG: Tracks POST-PP: {video_records_post_df['track_id'].nunique() if not video_records_post_df.empty else 0}.\")\n",
    "\n",
    "\n",
    "if not all_tracking_records_post:\n",
    "    print(\"ATTENTION: No data POST-PP found.\")\n",
    "else:\n",
    "    final_tracking_df = pd.DataFrame(all_tracking_records_post)\n",
    "\n",
    "    final_tracking_df = final_tracking_df[[\n",
    "        'video_id', 'frame_num', 'track_id',\n",
    "        'x_min', 'y_min', 'x_max', 'y_max' \n",
    "    ]].sort_values(by=['video_id', 'track_id', 'frame_num']).reset_index(drop=True)\n",
    "\n",
    "    print(f\" LOG: DataFrame POST-PP created (Total {len(final_tracking_df)} record).\")\n",
    "\n",
    "    save_tracking_results_to_csv(final_tracking_df, video_id=\"consolidated_post_processed\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JOmWfIPmCPhF"
   },
   "outputs": [],
   "source": [
    "TARGET_VIDEO_FILENAME = \"58102_002798_Endzone.mp4\"\n",
    "TARGET_VIDEO_FOLDER = TARGET_VIDEO_FILENAME.replace('.mp4', '')\n",
    "\n",
    "possible_roots = [\n",
    "    \"/content/datasets/Dataset/Videos_dataset/train\",\n",
    "    \"/content/datasets/Dataset/Videos_dataset/test\",\n",
    "    \"/content/datasets/Dataset/Videos_dataset\"\n",
    "]\n",
    "\n",
    "found_video_path = None\n",
    "for root in possible_roots:\n",
    "    path = os.path.join(root, TARGET_VIDEO_FILENAME)\n",
    "    if os.path.exists(path):\n",
    "        found_video_path = root\n",
    "        break\n",
    "\n",
    "if not found_video_path:\n",
    "    print(f\" ERROR: {TARGET_VIDEO_FILENAME}!\")\n",
    "    VIDEOS_DATA = \"/content/datasets/Dataset/Videos_dataset\"\n",
    "else:\n",
    "    VIDEOS_DATA = found_video_path\n",
    "\n",
    "if 'final_tracking_df' in locals():\n",
    "    df_source = final_tracking_df\n",
    "else:\n",
    "    print(\"Error.\")\n",
    "    df_source = pd.DataFrame()\n",
    "\n",
    "if not df_source.empty:\n",
    "    col_video = next((c for c in ['video_id', 'video_folder', 'video'] if c in df_source.columns), None)\n",
    "\n",
    "    if col_video:\n",
    "        df_target = df_source[df_source[col_video].astype(str).str.contains(TARGET_VIDEO_FOLDER)].copy()\n",
    "    else:\n",
    "        df_target = pd.DataFrame()\n",
    "\n",
    "    if not df_target.empty:\n",
    "        from collections import defaultdict\n",
    "        vis_tracks = defaultdict(list)\n",
    "\n",
    "        for _, row in df_target.iterrows():\n",
    "            frame = int(row.get('frame_num', row.get('frame')))\n",
    "            tid = int(row['track_id'])\n",
    "\n",
    "            if 'x_min' in row:\n",
    "                x1, y1 = int(row['x_min']), int(row['y_min'])\n",
    "                x2, y2 = int(row['x_max']), int(row['y_max'])\n",
    "            else:\n",
    "                x1, y1 = int(row['left']), int(row['top'])\n",
    "                x2 = int(x1 + row['width'])\n",
    "                y2 = int(y1 + row['height'])\n",
    "\n",
    "            vis_tracks[frame].append({'id': tid, 'bbox': [x1, y1, x2, y2]})\n",
    "\n",
    "        VIS_KEY = 'manual_check'\n",
    "        vis_wrapper = {VIS_KEY: vis_tracks}\n",
    "\n",
    "        visualize_and_save_video(\n",
    "            video_folder_name=TARGET_VIDEO_FOLDER,\n",
    "            fps=59.94,\n",
    "            tracking_data=vis_wrapper,\n",
    "            data_key=VIS_KEY,\n",
    "            suffix='_FINAL_CHECK'\n",
    "        )\n",
    "    else:\n",
    "        print(f\" No tracking data for: {TARGET_VIDEO_FILENAME}.\")\n",
    "else:\n",
    "    print(\" DataFrame Empty.\")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "name": "",
   "version": ""
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}

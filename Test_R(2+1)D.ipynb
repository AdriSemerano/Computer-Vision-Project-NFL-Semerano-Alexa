{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SGnnG6cZnkyx",
        "outputId": "55858131-b2e6-4db6-f724-50897f29d924"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: gdown in /usr/local/lib/python3.12/dist-packages (5.2.0)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.12/dist-packages (from gdown) (4.13.5)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from gdown) (3.20.0)\n",
            "Requirement already satisfied: requests[socks] in /usr/local/lib/python3.12/dist-packages (from gdown) (2.32.4)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from gdown) (4.67.1)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.12/dist-packages (from beautifulsoup4->gdown) (2.8)\n",
            "Requirement already satisfied: typing-extensions>=4.0.0 in /usr/local/lib/python3.12/dist-packages (from beautifulsoup4->gdown) (4.15.0)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests[socks]->gdown) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests[socks]->gdown) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests[socks]->gdown) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests[socks]->gdown) (2025.11.12)\n",
            "Requirement already satisfied: PySocks!=1.5.7,>=1.5.6 in /usr/local/lib/python3.12/dist-packages (from requests[socks]->gdown) (1.7.1)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (4.67.1)\n",
            "Collecting ultralytics\n",
            "  Downloading ultralytics-8.3.249-py3-none-any.whl.metadata (37 kB)\n",
            "Requirement already satisfied: numpy>=1.23.0 in /usr/local/lib/python3.12/dist-packages (from ultralytics) (2.0.2)\n",
            "Requirement already satisfied: matplotlib>=3.3.0 in /usr/local/lib/python3.12/dist-packages (from ultralytics) (3.10.0)\n",
            "Requirement already satisfied: opencv-python>=4.6.0 in /usr/local/lib/python3.12/dist-packages (from ultralytics) (4.12.0.88)\n",
            "Requirement already satisfied: pillow>=7.1.2 in /usr/local/lib/python3.12/dist-packages (from ultralytics) (11.3.0)\n",
            "Requirement already satisfied: pyyaml>=5.3.1 in /usr/local/lib/python3.12/dist-packages (from ultralytics) (6.0.3)\n",
            "Requirement already satisfied: requests>=2.23.0 in /usr/local/lib/python3.12/dist-packages (from ultralytics) (2.32.4)\n",
            "Requirement already satisfied: scipy>=1.4.1 in /usr/local/lib/python3.12/dist-packages (from ultralytics) (1.16.3)\n",
            "Requirement already satisfied: torch>=1.8.0 in /usr/local/lib/python3.12/dist-packages (from ultralytics) (2.9.0+cu126)\n",
            "Requirement already satisfied: torchvision>=0.9.0 in /usr/local/lib/python3.12/dist-packages (from ultralytics) (0.24.0+cu126)\n",
            "Requirement already satisfied: psutil>=5.8.0 in /usr/local/lib/python3.12/dist-packages (from ultralytics) (5.9.5)\n",
            "Requirement already satisfied: polars>=0.20.0 in /usr/local/lib/python3.12/dist-packages (from ultralytics) (1.31.0)\n",
            "Collecting ultralytics-thop>=2.0.18 (from ultralytics)\n",
            "  Downloading ultralytics_thop-2.0.18-py3-none-any.whl.metadata (14 kB)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.3.0->ultralytics) (1.3.3)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.3.0->ultralytics) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.3.0->ultralytics) (4.61.1)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.3.0->ultralytics) (1.4.9)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.3.0->ultralytics) (25.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.3.0->ultralytics) (3.2.5)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.3.0->ultralytics) (2.9.0.post0)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests>=2.23.0->ultralytics) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests>=2.23.0->ultralytics) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests>=2.23.0->ultralytics) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests>=2.23.0->ultralytics) (2025.11.12)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (3.20.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (4.15.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (1.14.0)\n",
            "Requirement already satisfied: networkx>=2.5.1 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (3.6.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (3.1.6)\n",
            "Requirement already satisfied: fsspec>=0.8.5 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (2025.3.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.5 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (2.27.5)\n",
            "Requirement already satisfied: nvidia-nvshmem-cu12==3.3.20 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (3.3.20)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.5.0 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (3.5.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.7->matplotlib>=3.3.0->ultralytics) (1.17.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch>=1.8.0->ultralytics) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch>=1.8.0->ultralytics) (3.0.3)\n",
            "Downloading ultralytics-8.3.249-py3-none-any.whl (1.2 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m26.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading ultralytics_thop-2.0.18-py3-none-any.whl (28 kB)\n",
            "Installing collected packages: ultralytics-thop, ultralytics\n",
            "Successfully installed ultralytics-8.3.249 ultralytics-thop-2.0.18\n",
            "Creating new Ultralytics Settings v0.0.6 file âœ… \n",
            "View Ultralytics Settings with 'yolo settings' or at '/root/.config/Ultralytics/settings.json'\n",
            "Update Settings with 'yolo settings key=value', i.e. 'yolo settings runs_dir=path/to/dir'. For help see https://docs.ultralytics.com/quickstart/#ultralytics-settings.\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "['/content/test/reid_checkpoints/phase1_reid_hybrid_ENDZONE_epoch_20.pth',\n",
              " '/content/test/reid_checkpoints/phase1_reid_hybrid_SIDELINE_epoch_20.pth',\n",
              " '/content/test/reid_checkpoints/reid_hybrid_ENDZONE_best.pth',\n",
              " '/content/test/reid_checkpoints/reid_hybrid_SIDELINE_best.pth',\n",
              " '/content/test/best_dualview_96.pth',\n",
              " '/content/test/best_weights_players_detection.pt',\n",
              " '/content/test/best.pt',\n",
              " '/content/test/best2.pt',\n",
              " '/content/test/ckpt.t7',\n",
              " '/content/test/helmet_dataset.yaml',\n",
              " '/content/test/helmets_extracted.zip',\n",
              " '/content/test/nfl_r2plus1d_Endzone_BEST_F1.pth',\n",
              " '/content/test/nfl_r2plus1d_Endzone_best.pth',\n",
              " '/content/test/nfl_r2plus1d_Sideline_BEST_F1.pth',\n",
              " '/content/test/nfl_r2plus1d_Sideline_BEST_F1(1).pth',\n",
              " '/content/test/nfl_r2plus1d_Sideline_best.pth',\n",
              " '/content/test/submission.csv',\n",
              " '/content/test/test.zip',\n",
              " '/content/test/train_labels.csv',\n",
              " '/content/test/xgb_threshold.txt',\n",
              " '/content/test/xgboost_impact_model.json']"
            ]
          },
          "execution_count": 2,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "!pip install gdown\n",
        "!pip install tqdm\n",
        "!pip install ultralytics\n",
        "\n",
        "\n",
        "import gdown          \n",
        "import os\n",
        "import shutil\n",
        "import zipfile\n",
        "from google.colab import files \n",
        "import random\n",
        "import itertools\n",
        "import math\n",
        "import xgboost as xgb\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from collections import defaultdict\n",
        "from collections import deque\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torchvision.models.video import r2plus1d_18, R2Plus1D_18_Weights\n",
        "import torch.nn.functional as F\n",
        "import torchvision.models as models\n",
        "from torch.utils.data import Dataset, WeightedRandomSampler, DataLoader, SubsetRandomSampler\n",
        "from torchvision import transforms\n",
        "import torchvision.transforms as T\n",
        "import torch.optim as optim\n",
        "from sklearn.model_selection import train_test_split\n",
        "from ultralytics import YOLO\n",
        "\n",
        "import cv2\n",
        "from tqdm import tqdm \n",
        "from PIL import Image\n",
        "from IPython.display import display, FileLink, Markdown\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "from scipy.optimize import linear_sum_assignment\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "url_videos = \"https://drive.google.com/drive/folders/1w28fjrEBie5k1MbG14Dw25Zvj_jxJ6cE?usp=sharing\"\n",
        "gdown.download_folder(url_videos, quiet=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UFudMbtjqrCB",
        "outputId": "6d3ecefa-0416-4e98-e4d1-99f5eb52a96d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "âœ… Cartella 'test' spostata in 'datasets/Dataset'\n",
            "File zip datasets/Dataset/test.zip estratto e rimosso.\n",
            "File zip datasets/Dataset/helmets_extracted.zip estratto e rimosso.\n"
          ]
        }
      ],
      "source": [
        "\n",
        "original_path = \"test\"\n",
        "new_base = \"datasets\"\n",
        "new_path = os.path.join(new_base, \"Dataset\")\n",
        "\n",
        "os.makedirs(new_base, exist_ok=True)\n",
        "\n",
        "if not os.path.exists(new_path):\n",
        "    shutil.move(original_path, new_path)\n",
        "    print(f\" Folder '{original_path}' moved in '{new_path}'\")\n",
        "dataset_path = \"datasets/Dataset\"\n",
        "\n",
        "# extract zip files and remove them\n",
        "def extract_and_remove_zip(zip_path, extract_to):\n",
        "    with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
        "        zip_ref.extractall(extract_to)\n",
        "    os.remove(zip_path)\n",
        "    print(f\"File zip {zip_path} extracted and removed.\")\n",
        "\n",
        "zip_files = [f for f in os.listdir(dataset_path) if f.endswith('.zip')]\n",
        "for zip_file in zip_files:\n",
        "    zip_path = os.path.join(dataset_path, zip_file)\n",
        "    extract_and_remove_zip(zip_path, dataset_path)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "upKWnXs2ec8q"
      },
      "source": [
        "# ***Detection YOLO***"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mdW-z155s8sP",
        "outputId": "214e4e77-20fd-41d0-b103-ebe8f6e82a96"
      },
      "outputs": [],
      "source": [
        "def get_helmet_detections(video_dir, model_path, conf_conf=0.2):\n",
        "\n",
        "\n",
        "    print(f\" Yolo Detection on video in: {video_dir}\")\n",
        "\n",
        "    model = YOLO(model_path)\n",
        "\n",
        "    all_detections = []\n",
        "\n",
        "    video_files = [f for f in os.listdir(video_dir) if f.endswith((\".mp4\", \".avi\", \".mov\"))]\n",
        "\n",
        "    for video_name in video_files:\n",
        "        video_path = os.path.join(video_dir, video_name)\n",
        "        cap = cv2.VideoCapture(video_path)\n",
        "\n",
        "        total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
        "\n",
        "        frame_num = 1\n",
        "\n",
        "        pbar = tqdm(total=total_frames, desc=f\"ðŸ‘€ {video_name}\", unit=\"fr\", leave=False)\n",
        "\n",
        "        while True:\n",
        "            ret, frame = cap.read()\n",
        "            if not ret:\n",
        "                break\n",
        "\n",
        "            results = model(frame, verbose=False)[0]\n",
        "\n",
        "            if results.boxes is not None:\n",
        "                boxes = results.boxes.xyxy.cpu().numpy()\n",
        "                confs = results.boxes.conf.cpu().numpy()\n",
        "\n",
        "                for box, conf in zip(boxes, confs):\n",
        "                    if conf > conf_conf:\n",
        "                        x1, y1, x2, y2 = box.astype(int)\n",
        "\n",
        "                        # Salviamo nella struttura leggera\n",
        "                        all_detections.append({\n",
        "                            'video_file': video_name,\n",
        "                            'frame_num': frame_num,\n",
        "                            'x1': x1,\n",
        "                            'y1': y1,\n",
        "                            'x2': x2,\n",
        "                            'y2': y2,\n",
        "                            'conf': float(conf)\n",
        "                        })\n",
        "\n",
        "            frame_num += 1\n",
        "            pbar.update(1)\n",
        "\n",
        "        cap.release()\n",
        "        pbar.close()\n",
        "\n",
        "    df_detections = pd.DataFrame(all_detections)\n",
        "\n",
        "    if not df_detections.empty:\n",
        "        df_detections = df_detections.sort_values(by=['video_file', 'frame_num']).reset_index(drop=True)\n",
        "\n",
        "    print(f\"\\n Detection complete. Found {len(df_detections)} helmets.\")\n",
        "    return df_detections\n",
        "\n",
        "VIDEO_DIR = \"/content/datasets/Dataset/test\"\n",
        "MODEL_PATH = \"/content/datasets/Dataset/best.pt\"\n",
        "\n",
        "df_raw_detections = get_helmet_detections(VIDEO_DIR, MODEL_PATH)\n",
        "\n",
        "print(df_raw_detections.head())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L2302aeyePHx"
      },
      "source": [
        "# ***Sostituzione YOLO con CSV***"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iKhwVrDYeOK6",
        "outputId": "45df8016-c007-45a8-d3b9-81f73ed55eba"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "âŒ ERRORE: File GT non trovato in /content/datasets/Dataset/train_labels.csv\n"
          ]
        }
      ],
      "source": [
        "GT = True\n",
        "\n",
        "if os.path.exists(GT_PATH) and GT:\n",
        "    # 1. Caricamento del file train_labels.csv\n",
        "    df_gt = pd.read_csv(GT_PATH)\n",
        "\n",
        "    # 2. Identificazione dei video da processare\n",
        "    # Se hai una lista di video specifica (es. quella usata per YOLO), usala qui.\n",
        "    # Altrimenti, prendiamo i video presenti nella cartella di test.\n",
        "    test_videos = [f for f in os.listdir(VIDEO_DIR) if f.endswith('.mp4')]\n",
        "\n",
        "    # 3. Filtraggio GT solo per i video presenti nella cartella di test\n",
        "    df_raw_detections = df_gt[df_gt['video'].isin(test_videos)].copy()\n",
        "\n",
        "    if df_raw_detections.empty:\n",
        "        print(\"âš ï¸ ATTENZIONE: Nessuna corrispondenza trovata tra i video in VIDEO_DIR e il file GT!\")\n",
        "    else:\n",
        "        # 4. Conversione formato: GT (left, top, width, height) -> Pipeline (x1, y1, x2, y2)\n",
        "        df_raw_detections['x1'] = df_raw_detections['left']\n",
        "        df_raw_detections['y1'] = df_raw_detections['top']\n",
        "        df_raw_detections['x2'] = df_raw_detections['left'] + df_raw_detections['width']\n",
        "        df_raw_detections['y2'] = df_raw_detections['top'] + df_raw_detections['height']\n",
        "\n",
        "        # 5. Allineamento nomi colonne\n",
        "        df_raw_detections = df_raw_detections.rename(columns={\n",
        "            'video': 'video_file',\n",
        "            'frame': 'frame_num'\n",
        "        })\n",
        "\n",
        "        # 6. Pulizia colonne extra per pulizia (manteniamo solo quelle necessarie)\n",
        "        cols_to_keep = ['video_file', 'frame_num', 'x1', 'y1', 'x2', 'y2']\n",
        "        # Teniamo anche la label originale (es. H30) per debug avanzato se serve\n",
        "        if 'label' in df_raw_detections.columns:\n",
        "            cols_to_keep.append('label')\n",
        "\n",
        "        df_raw_detections = df_raw_detections[cols_to_keep].reset_index(drop=True)\n",
        "\n",
        "        print(f\"âœ… Struttura 'df_raw_detections' aggiornata con {len(df_raw_detections)} box GT.\")\n",
        "        print(f\"   Video target rilevati: {df_raw_detections['video_file'].nunique()}\")\n",
        "else:\n",
        "    if os.path.exists(GT_PATH):\n",
        "        print(f\"âŒ ERRORE: File GT non trovato in {GT_PATH}\")\n",
        "    else:\n",
        "        print(\"Usiamo le detection di YOLO\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Oey0gagceX_p"
      },
      "source": [
        "# ***EXTRACTING CROPS***"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CLepvS7ZGrCk",
        "outputId": "3f8ecc1a-66a0-4b43-9663-cb47d7320374"
      },
      "outputs": [],
      "source": [
        "VIDEO_DIR = \"/content/datasets/Dataset/test\"\n",
        "OUT_DIR_REID = \"/content/datasets/Dataset/temp_crops_reid\"       # 64x64\n",
        "OUT_DIR_CONTEXT = \"/content/datasets/Dataset/temp_crops_context\" # 128x128 (3x context)\n",
        "\n",
        "os.makedirs(OUT_DIR_REID, exist_ok=True)\n",
        "os.makedirs(OUT_DIR_CONTEXT, exist_ok=True)\n",
        "\n",
        "\n",
        "SIZE_REID = 64\n",
        "SIZE_CONTEXT = 128\n",
        "CONTEXT_FACTOR = 3\n",
        "\n",
        "def extract_crops_from_detections(df_detections, video_dir):\n",
        "    print(f\"Extraction crop from {len(df_detections)} detection YOLO...\")\n",
        "\n",
        "    df_detections['crop_path_reid'] = None\n",
        "    df_detections['crop_path_context'] = None\n",
        "\n",
        "    unique_videos = df_detections['video_file'].unique()\n",
        "\n",
        "    for video_name in tqdm(unique_videos, desc=\"Estrazione Video\"):\n",
        "        video_path = os.path.join(video_dir, video_name)\n",
        "        cap = cv2.VideoCapture(video_path)\n",
        "\n",
        "        if not cap.isOpened():\n",
        "            print(f\"âš ï¸ Impossibile aprire {video_name}\")\n",
        "            continue\n",
        "\n",
        "        vid_dets = df_detections[df_detections['video_file'] == video_name].copy().sort_values('frame_num')\n",
        "        frames_with_dets = vid_dets.groupby('frame_num')\n",
        "\n",
        "        for frame_idx, group in frames_with_dets:\n",
        "            cap.set(cv2.CAP_PROP_POS_FRAMES, frame_idx - 1)\n",
        "            ret, frame = cap.read()\n",
        "\n",
        "            if not ret:\n",
        "                continue\n",
        "\n",
        "            h_img, w_img = frame.shape[:2]\n",
        "\n",
        "            for idx, row in group.iterrows():\n",
        "                x1, y1, x2, y2 = int(row['x1']), int(row['y1']), int(row['x2']), int(row['y2'])\n",
        "\n",
        "                x1_c = max(0, x1); y1_c = max(0, y1)\n",
        "                x2_c = min(w_img, x2); y2_c = min(h_img, y2)\n",
        "\n",
        "                crop_tight = frame[y1_c:y2_c, x1_c:x2_c]\n",
        "\n",
        "                if crop_tight.size == 0: continue\n",
        "\n",
        "                crop_tight_resized = cv2.resize(crop_tight, (SIZE_REID, SIZE_REID))\n",
        "                filename = f\"{video_name.replace('.mp4','')}_{frame_idx}_{idx}.jpg\"\n",
        "\n",
        "                path_reid = os.path.join(OUT_DIR_REID, filename)\n",
        "                cv2.imwrite(path_reid, crop_tight_resized)\n",
        "\n",
        "                w_box = x2 - x1\n",
        "                h_box = y2 - y1\n",
        "                cx = x1 + w_box / 2\n",
        "                cy = y1 + h_box / 2\n",
        "\n",
        "                w_amp = w_box * CONTEXT_FACTOR\n",
        "                h_amp = h_box * CONTEXT_FACTOR\n",
        "\n",
        "                x1_amp = int(cx - w_amp / 2)\n",
        "                y1_amp = int(cy - h_amp / 2)\n",
        "                x2_amp = int(cx + w_amp / 2)\n",
        "                y2_amp = int(cy + h_amp / 2)\n",
        "\n",
        "                x1_amp = max(0, x1_amp); y1_amp = max(0, y1_amp)\n",
        "                x2_amp = min(w_img, x2_amp); y2_amp = min(h_img, y2_amp)\n",
        "\n",
        "                crop_context = frame[y1_amp:y2_amp, x1_amp:x2_amp]\n",
        "\n",
        "                if crop_context.size > 0:\n",
        "                    # Resize 128x128\n",
        "                    crop_context_resized = cv2.resize(crop_context, (SIZE_CONTEXT, SIZE_CONTEXT))\n",
        "\n",
        "                    path_context = os.path.join(OUT_DIR_CONTEXT, filename)\n",
        "                    cv2.imwrite(path_context, crop_context_resized)\n",
        "\n",
        "                    df_detections.at[idx, 'crop_path_reid'] = path_reid\n",
        "                    df_detections.at[idx, 'crop_path_context'] = path_context\n",
        "                    df_detections.at[idx, 'x1_amp'] = x1_amp\n",
        "                    df_detections.at[idx, 'y1_amp'] = y1_amp\n",
        "                    df_detections.at[idx, 'x2_amp'] = x2_amp\n",
        "                    df_detections.at[idx, 'y2_amp'] = y2_amp\n",
        "\n",
        "        cap.release()\n",
        "    return df_detections\n",
        "\n",
        "if 'df_raw_detections' in locals() and not df_raw_detections.empty:\n",
        "    df_with_crops = extract_crops_from_detections(df_raw_detections, VIDEO_DIR)\n",
        "\n",
        "\n",
        "    df_with_crops = df_with_crops.dropna(subset=['crop_path_reid', 'crop_path_context'])\n",
        "\n",
        "    print(\"\\nData Ready for Tracking:\")\n",
        "    print(df_with_crops[['video_file', 'frame_num', 'crop_path_reid']].head())\n",
        "else:\n",
        "    print(\" Error\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0IVM2ybwb-YM"
      },
      "source": [
        "# ***TRACKING***"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 431
        },
        "id": "2tabiiqE6JAY",
        "outputId": "1bd2a64c-2805-413d-a2d4-3a3d446fdb05"
      },
      "outputs": [],
      "source": [
        "\n",
        "ROOT_DIR = \"/content/datasets/Dataset\"\n",
        "CHECKPOINT_DIR = os.path.join(ROOT_DIR, \"reid_checkpoints\")\n",
        "VIDEO_DIR = \"/content/datasets/Dataset/test\"\n",
        "\n",
        "MAX_COSINE_DISTANCE = 0.70\n",
        "MAX_IOU_DISTANCE = 0.85\n",
        "WEIGHT_REID = 0.30\n",
        "\n",
        "WEIGHT_IOU = 0.70\n",
        "\n",
        "MAX_AGE = 150\n",
        "REID_THRESHOLD_REACQ = 0.60\n",
        "\n",
        "N_INIT = 3\n",
        "MIN_TRACK_LENGTH = 15\n",
        "\n",
        "FRAME_W, FRAME_H = 1280, 720\n",
        "EMBEDDING_DIM = 128\n",
        "POSITIONAL_FEATURES_DIM = 4\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "EXTRACTORS = {}\n",
        "TRACK_EMBEDDINGS_CACHE = defaultdict(lambda: defaultdict(dict))\n",
        "\n",
        "\n",
        "class HybridFrameReIDModel(nn.Module):\n",
        "    def __init__(self, embedding_dim=128, positional_features_dim=4):\n",
        "        super().__init__()\n",
        "        resnet = models.resnet18(weights=models.ResNet18_Weights.IMAGENET1K_V1)\n",
        "        self.features = nn.Sequential(*(list(resnet.children())[:-2]))\n",
        "        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n",
        "        VISUAL_FEATURE_DIM = 512\n",
        "        COMBINED_DIM = VISUAL_FEATURE_DIM + positional_features_dim\n",
        "        self.embedding_layer = nn.Linear(COMBINED_DIM, embedding_dim)\n",
        "\n",
        "    def forward(self, x_image, x_positional):\n",
        "        x_image = self.features(x_image)\n",
        "        x_image = self.avgpool(x_image)\n",
        "        x_image = torch.flatten(x_image, 1)\n",
        "        combined_features = torch.cat((x_image, x_positional), dim=1)\n",
        "        embedding = self.embedding_layer(combined_features)\n",
        "        return nn.functional.normalize(embedding, p=2, dim=1)\n",
        "\n",
        "transform = transforms.Compose([\n",
        "    transforms.ToPILImage(), transforms.Resize((64, 64)), transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "])\n",
        "\n",
        "class CustomFeatureExtractor:\n",
        "    def __init__(self, reid_model, transform, device, frame_w, frame_h):\n",
        "        self.reid_model = reid_model\n",
        "        self.transform = transform\n",
        "        self.device = device\n",
        "        self.FRAME_W, self.FRAME_H = frame_w, frame_h\n",
        "        self.reid_model.eval()\n",
        "\n",
        "    def __call__(self, img_crops, bbox_xywh_abs):\n",
        "        if len(img_crops) == 0:\n",
        "            return np.empty((0, self.reid_model.embedding_layer.out_features), dtype=np.float32)\n",
        "        image_tensors = [self.transform(cv2.cvtColor(c, cv2.COLOR_BGR2RGB)) if c is not None and c.size > 0\n",
        "                         else torch.zeros(3, 64, 64) for c in img_crops]\n",
        "        image_batch = torch.stack(image_tensors).to(self.device)\n",
        "        pos_batch = torch.tensor(bbox_xywh_abs / [self.FRAME_W, self.FRAME_H, self.FRAME_W, self.FRAME_H],\n",
        "                                 dtype=torch.float32).to(self.device)\n",
        "        with torch.no_grad():\n",
        "            return self.reid_model(image_batch, pos_batch).cpu().numpy()\n",
        "\n",
        "def get_feature_extractor(view_type):\n",
        "    global EXTRACTORS\n",
        "    if view_type in EXTRACTORS: return EXTRACTORS[view_type]\n",
        "    model = HybridFrameReIDModel(EMBEDDING_DIM, POSITIONAL_FEATURES_DIM).to(device)\n",
        "    ckpt = \"reid_hybrid_SIDELINE_best.pth\" if 'sideline' in view_type.lower() else \"reid_hybrid_ENDZONE_best.pth\"\n",
        "    path = os.path.join(CHECKPOINT_DIR, ckpt)\n",
        "    if os.path.exists(path):\n",
        "        sd = torch.load(path, map_location=device)\n",
        "        model.load_state_dict(sd['model_state_dict'] if 'model_state_dict' in sd else sd, strict=False)\n",
        "        print(f\"â„¹ï¸ Estrattore {view_type} caricato.\")\n",
        "    EXTRACTORS[view_type] = CustomFeatureExtractor(model, transform, device, FRAME_W, FRAME_H)\n",
        "    return EXTRACTORS[view_type]\n",
        "\n",
        "class SimpleTrack:\n",
        "    _count = 0\n",
        "    def __init__(self, embedding, bbox_abs, frame_num):\n",
        "        SimpleTrack._count += 1\n",
        "        self.track_id = SimpleTrack._count\n",
        "        self.last_bbox_abs = bbox_abs.copy()\n",
        "        self.embeddings = deque([embedding], maxlen=30)\n",
        "        self.hits, self.time_since_update = 1, 0\n",
        "        self.deleted = False\n",
        "\n",
        "    def update(self, embedding, bbox_abs):\n",
        "        self.embeddings.append(embedding)\n",
        "        self.last_bbox_abs, self.hits, self.time_since_update = bbox_abs.copy(), self.hits + 1, 0\n",
        "\n",
        "    def mark_missed(self):\n",
        "        self.time_since_update += 1\n",
        "        if self.time_since_update > MAX_AGE: self.deleted = True\n",
        "\n",
        "    def is_deleted(self): return self.deleted\n",
        "\n",
        "    @property\n",
        "    def predicted_bbox_tlbr(self):\n",
        "        xc, yc, w, h = self.last_bbox_abs\n",
        "        return np.array([xc - w/2, yc - h/2, xc + w/2, yc + h/2])\n",
        "\n",
        "    @property\n",
        "    def mean_embedding(self):\n",
        "        m = np.mean(self.embeddings, axis=0)\n",
        "        return (m / (np.linalg.norm(m) + 1e-6)).reshape(1, -1)\n",
        "\n",
        "\n",
        "def bbox_iou_simple(boxA, boxB):\n",
        "    xA, yA = max(boxA[0], boxB[0]), max(boxA[1], boxB[1])\n",
        "    xB, yB = min(boxA[2], boxB[2]), min(boxA[3], boxB[3])\n",
        "    inter = max(0, xB - xA) * max(0, yB - yA)\n",
        "    areaA = (boxA[2]-boxA[0]) * (boxA[3]-boxA[1])\n",
        "    areaB = (boxB[2]-boxB[0]) * (boxB[3]-boxB[1])\n",
        "    union = float(areaA + areaB - inter)\n",
        "    return inter / union if union > 0 else 0.0\n",
        "\n",
        "def associate_detections_to_tracks(all_tracks, new_detections_data, extractor):\n",
        "    active_tracks = [t for t in all_tracks if not t.is_deleted() and t.time_since_update < 5]\n",
        "    lost_tracks = [t for t in all_tracks if not t.is_deleted() and t.time_since_update >= 5]\n",
        "\n",
        "    if not all_tracks or not new_detections_data:\n",
        "        return [], [t for t in all_tracks if not t.is_deleted()], list(range(len(new_detections_data)))\n",
        "\n",
        "    det_crops = [d[1] for d in new_detections_data]\n",
        "    det_boxes_abs = np.vstack([d[2] for d in new_detections_data])\n",
        "    det_embs = extractor(det_crops, det_boxes_abs)\n",
        "\n",
        "    matches = []\n",
        "    unmatched_dets = list(range(len(new_detections_data)))\n",
        "\n",
        "    if active_tracks:\n",
        "        track_boxes = np.array([t.predicted_bbox_tlbr for t in active_tracks])\n",
        "        track_embs = np.vstack([t.mean_embedding for t in active_tracks])\n",
        "\n",
        "        sim_matrix = cosine_similarity(det_embs, track_embs)\n",
        "        reid_cost = 1.0 - sim_matrix\n",
        "\n",
        "        det_boxes_tlbr = np.array([[d[0][0], d[0][1], d[0][0]+d[0][2], d[0][1]+d[0][3]] for d in new_detections_data])\n",
        "        iou_cost = np.array([[1.0 - bbox_iou_simple(d, t) for t in track_boxes] for d in det_boxes_tlbr])\n",
        "\n",
        "        cost_matrix = (WEIGHT_REID * reid_cost) + (WEIGHT_IOU * iou_cost)\n",
        "        cost_matrix[iou_cost > MAX_IOU_DISTANCE] = 1.0\n",
        "\n",
        "        temp_dets = unmatched_dets.copy()\n",
        "        while temp_dets and active_tracks:\n",
        "            sub = cost_matrix[np.ix_(temp_dets, range(len(active_tracks)))]\n",
        "            i, j = np.unravel_index(sub.argmin(), sub.shape)\n",
        "            if sub[i, j] > MAX_COSINE_DISTANCE: break\n",
        "\n",
        "            det_idx = temp_dets.pop(i)\n",
        "            matches.append((det_idx, active_tracks[j]))\n",
        "            unmatched_dets.remove(det_idx)\n",
        "\n",
        "    if lost_tracks and unmatched_dets:\n",
        "        lost_embs = np.vstack([t.mean_embedding for t in lost_tracks])\n",
        "        remaining_det_embs = det_embs[unmatched_dets]\n",
        "\n",
        "        reacq_sim = cosine_similarity(remaining_det_embs, lost_embs)\n",
        "\n",
        "        for i, det_idx in enumerate(unmatched_dets.copy()):\n",
        "            best_trk_idx = reacq_sim[i].argmax()\n",
        "            if reacq_sim[i][best_trk_idx] > (1.0 - REID_THRESHOLD_REACQ):\n",
        "                matches.append((det_idx, lost_tracks[best_trk_idx]))\n",
        "                unmatched_dets.remove(det_idx)\n",
        "\n",
        "    return matches, [t for t in all_tracks if not t.is_deleted()], unmatched_dets\n",
        "\n",
        "\n",
        "def run_tracking_with_logs(video_name, df_video_crops):\n",
        "    view_type = 'SIDELINE' if 'Sideline' in video_name else 'ENDZONE'\n",
        "    extractor = get_feature_extractor(view_type)\n",
        "    video_key = video_name.replace('.mp4', '')\n",
        "    all_tracks, tracking_records = [], []\n",
        "    frames_group = df_video_crops.sort_values('frame_num').groupby('frame_num')\n",
        "\n",
        "    for frame_idx, frame_data in frames_group:\n",
        "        new_dets = []\n",
        "        for idx, row in frame_data.iterrows():\n",
        "            x1, y1, x2, y2 = int(row['x1']), int(row['y1']), int(row['x2']), int(row['y2'])\n",
        "            crop = cv2.imread(row['crop_path_reid'])\n",
        "            if crop is not None:\n",
        "                w, h = x2 - x1, y2 - y1\n",
        "                new_dets.append((np.array([x1, y1, w, h]), crop, np.array([x1+w/2, y1+h/2, w, h]), idx))\n",
        "\n",
        "        matches, current_active_tracks, unmatched = associate_detections_to_tracks(all_tracks, new_dets, extractor)\n",
        "\n",
        "        matched_set = set()\n",
        "        for d_idx, track in matches:\n",
        "            bbox, crop, bbox_abs, orig_idx = new_dets[d_idx]\n",
        "            emb = extractor([crop], np.array([bbox_abs]))[0]\n",
        "            track.update(emb, bbox_abs)\n",
        "            matched_set.add(track)\n",
        "            tracking_records.append({\n",
        "                'video_id': video_key, 'frame_num': frame_idx, 'track_id': track.track_id,\n",
        "                'x_min': int(bbox[0]), 'y_min': int(bbox[1]), 'width': int(bbox[2]), 'height': int(bbox[3]),\n",
        "                'crop_path_context': df_video_crops.loc[orig_idx, 'crop_path_context']\n",
        "            })\n",
        "\n",
        "        for d_idx in unmatched:\n",
        "            bbox, crop, bbox_abs, orig_idx = new_dets[d_idx]\n",
        "            emb = extractor([crop], np.array([bbox_abs]))[0]\n",
        "            nt = SimpleTrack(emb, bbox_abs, frame_idx)\n",
        "            all_tracks.append(nt)\n",
        "            tracking_records.append({\n",
        "                'video_id': video_key, 'frame_num': frame_idx, 'track_id': nt.track_id,\n",
        "                'x_min': int(bbox[0]), 'y_min': int(bbox[1]), 'width': int(bbox[2]), 'height': int(bbox[3]),\n",
        "                'crop_path_context': df_video_crops.loc[orig_idx, 'crop_path_context']\n",
        "            })\n",
        "\n",
        "        for t in all_tracks:\n",
        "            if t not in matched_set: t.mark_missed()\n",
        "\n",
        "    return pd.DataFrame(tracking_records)\n",
        "\n",
        "if 'df_with_crops' in locals() and not df_with_crops.empty:\n",
        "    all_tracking_results = []\n",
        "    video_list = df_with_crops['video_file'].unique()\n",
        "\n",
        "    for i, video_name in enumerate(video_list):\n",
        "        print(f\"\\nProcessing {i+1}/{len(video_list)}: {video_name}\")\n",
        "        df_video_subset = df_with_crops[df_with_crops['video_file'] == video_name].copy()\n",
        "        df_track_raw = run_tracking_with_logs(video_name, df_video_subset)\n",
        "\n",
        "        if not df_track_raw.empty:\n",
        "            track_lengths = df_track_raw.groupby('track_id')['frame_num'].nunique()\n",
        "            df_track_raw = df_track_raw[df_track_raw['track_id'].isin(track_lengths[track_lengths >= MIN_TRACK_LENGTH].index)].copy()\n",
        "            final_count = df_track_raw['track_id'].nunique()\n",
        "            print(f\" Tracking complete. Final Tracks: {final_count}\")\n",
        "            all_tracking_results.append(df_track_raw)\n",
        "\n",
        "    if all_tracking_results:\n",
        "        final_tracks_df = pd.concat(all_tracking_results, ignore_index=True)\n",
        "        final_tracks_df.to_csv(\"final_test_tracks_tracking_log.csv\", index=False)\n",
        "        files.download(\"final_test_tracks_tracking_log.csv\")\n",
        "        final_consolidated_df = final_tracks_df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pb4JntuocIX8"
      },
      "source": [
        "# ***GENERATE SEQUENCES***"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cJecTd-qWoBM",
        "outputId": "47def36b-8db4-4fed-f0f8-a4910dca2b91"
      },
      "outputs": [],
      "source": [
        "GT_PATH = \"/content/datasets/Dataset/train_labels.csv\"    \n",
        "IOU_THRESHOLD = 0.35                     \n",
        "FRAME_TOLERANCE = 4                      # +/- 4 frame\n",
        "\n",
        "def bbox_iou(box1, box2):\n",
        "    x1 = max(box1['left'], box2['left'])\n",
        "    y1 = max(box1['top'], box2['top'])\n",
        "    x2 = min(box1['left'] + box1['width'], box2['left'] + box2['width'])\n",
        "    y2 = min(box1['top'] + box1['height'], box2['top'] + box2['height'])\n",
        "\n",
        "    intersection = max(0, x2 - x1) * max(0, y2 - y1)\n",
        "\n",
        "    area1 = box1['width'] * box1['height']\n",
        "    area2 = box2['width'] * box2['height']\n",
        "\n",
        "    union = area1 + area2 - intersection\n",
        "\n",
        "    if union <= 0: return 0.0\n",
        "    return intersection / union\n",
        "\n",
        "def evaluate_performance(pred_csv, gt_csv):\n",
        "    preds = pd.read_csv(pred_csv)\n",
        "    gt = pd.read_csv(gt_csv)\n",
        "    test_videos = preds['video'].unique()\n",
        "    gt = gt[gt['video'].isin(test_videos)].copy()\n",
        "\n",
        "    if 'confidence' in gt.columns:\n",
        "        gt = gt[(gt['impact'] == 1) & (gt['confidence'] > 1) & (gt['visibility'] > 0)]\n",
        "    else:\n",
        "        gt = gt[gt['impact'] == 1]\n",
        "\n",
        "    gt['matched'] = False\n",
        "    preds['is_tp'] = False\n",
        "\n",
        "    video_results = []\n",
        "\n",
        "    for video in test_videos:\n",
        "        v_preds = preds[preds['video'] == video].copy()\n",
        "        v_gt = gt[gt['video'] == video].copy()\n",
        "\n",
        "        v_tp = 0\n",
        "        v_fp = 0\n",
        "\n",
        "        if 'score' in v_preds.columns:\n",
        "            v_preds = v_preds.sort_values('score', ascending=False)\n",
        "\n",
        "        for idx, pred_row in v_preds.iterrows():\n",
        "            candidates = v_gt[\n",
        "                (v_gt['frame'] >= pred_row['frame'] - FRAME_TOLERANCE) &\n",
        "                (v_gt['frame'] <= pred_row['frame'] + FRAME_TOLERANCE) &\n",
        "                (v_gt['matched'] == False)\n",
        "            ]\n",
        "\n",
        "            best_iou = 0\n",
        "            best_match_idx = -1\n",
        "            pred_box = {'left': pred_row['left'], 'top': pred_row['top'],\n",
        "                        'width': pred_row['width'], 'height': pred_row['height']}\n",
        "\n",
        "            for gt_idx, gt_row in candidates.iterrows():\n",
        "                gt_box = {'left': gt_row['left'], 'top': gt_row['top'],\n",
        "                          'width': gt_row['width'], 'height': gt_row['height']}\n",
        "\n",
        "                iou = bbox_iou(pred_box, gt_box) \n",
        "                if iou > best_iou:\n",
        "                    best_iou = iou\n",
        "                    best_match_idx = gt_idx\n",
        "\n",
        "            if best_iou >= IOU_THRESHOLD:\n",
        "                v_tp += 1\n",
        "                v_gt.at[best_match_idx, 'matched'] = True\n",
        "                gt.at[best_match_idx, 'matched'] = True\n",
        "            else:\n",
        "                v_fp += 1\n",
        "\n",
        "        v_fn = len(v_gt[v_gt['matched'] == False])\n",
        "\n",
        "        v_precision = v_tp / (v_tp + v_fp) if (v_tp + v_fp) > 0 else 0.0\n",
        "        v_recall = v_tp / (v_tp + v_fn) if (v_tp + v_fn) > 0 else 0.0\n",
        "        v_f1 = 2 * (v_precision * v_recall) / (v_precision + v_recall) if (v_precision + v_recall) > 0 else 0.0\n",
        "\n",
        "        video_results.append({\n",
        "            'Video': video,\n",
        "            'GT_Count': len(v_gt),\n",
        "            'Pred_Count': len(v_preds),\n",
        "            'TP': v_tp, 'FP': v_fp, 'FN': v_fn,\n",
        "            'Precision': round(v_precision, 4),\n",
        "            'Recall': round(v_recall, 4),\n",
        "            'F1': round(v_f1, 4)\n",
        "        })\n",
        "\n",
        "    results_df = pd.DataFrame(video_results)\n",
        "\n",
        "    total_tp = results_df['TP'].sum()\n",
        "    total_fp = results_df['FP'].sum()\n",
        "    total_fn = results_df['FN'].sum()\n",
        "\n",
        "    global_precision = total_tp / (total_tp + total_fp) if (total_tp + total_fp) > 0 else 0.0\n",
        "    global_recall = total_tp / (total_tp + total_fn) if (total_tp + total_fn) > 0 else 0.0\n",
        "    global_f1 = 2 * (global_precision * global_recall) / (global_precision + global_recall) if (global_precision + global_recall) > 0 else 0.0\n",
        "\n",
        "    print(\"\\n\" + \"=\"*85)\n",
        "    print(f\"{'VIDEO RESULTS':^85}\")\n",
        "    print(\"=\"*85)\n",
        "    print(results_df.to_string(index=False))\n",
        "    print(\"-\" * 85)\n",
        "    print(f\"{'OVERALL':<30} TP: {total_tp:<5} FP: {total_fp:<5} FN: {total_fn:<5}\")\n",
        "    print(f\"{'':<30} Precision: {global_precision:.4f} Recall: {global_recall:.4f} F1: {global_f1:.4f}\")\n",
        "    print(\"=\"*85)\n",
        "\n",
        "    return results_df\n",
        "\n",
        "def extract_inference_sequences(df_tracks):\n",
        "    print(\"\\n Start extraction for Testing...\")\n",
        "\n",
        "    print(f\"   Column in DataFrame: {list(df_tracks.columns)}\")\n",
        "\n",
        "    if 'x_min' in df_tracks.columns and 'left' not in df_tracks.columns:\n",
        "        df_tracks['left'] = df_tracks['x_min']\n",
        "    if 'y_min' in df_tracks.columns and 'top' not in df_tracks.columns:\n",
        "        df_tracks['top'] = df_tracks['y_min']\n",
        "    if 'width' not in df_tracks.columns:\n",
        "        if 'x_max' in df_tracks.columns:\n",
        "            df_tracks['width'] = df_tracks['x_max'] - df_tracks['left']\n",
        "        else:\n",
        "            raise KeyError(\"can't compute width.\")\n",
        "\n",
        "    if 'height' not in df_tracks.columns:\n",
        "        if 'y_max' in df_tracks.columns:\n",
        "            df_tracks['height'] = df_tracks['y_max'] - df_tracks['top']\n",
        "        else:\n",
        "            raise KeyError(\"can't compute height\")\n",
        "\n",
        "    SAMPLE_LENGTH = 16\n",
        "    FRAME_STRIDE = 4\n",
        "    MAX_SPAN_ALLOWED = 25\n",
        "\n",
        "    inference_samples = []\n",
        "\n",
        "    df_tracks = df_tracks.sort_values(by=['video_id', 'track_id', 'frame_num'])\n",
        "\n",
        "    grouped = df_tracks.groupby(['video_id', 'track_id'])\n",
        "\n",
        "    for (video_id, track_id), group in tqdm(grouped, desc=\"Sequencing Tracks\"):\n",
        "\n",
        "        group = group.reset_index(drop=True)\n",
        "        frames = group['frame_num'].values\n",
        "\n",
        "        if len(frames) < SAMPLE_LENGTH:\n",
        "            continue\n",
        "\n",
        "        view_type = 'Endzone' if 'Endzone' in video_id else 'Sideline'\n",
        "\n",
        "        for i in range(0, len(frames) - SAMPLE_LENGTH + 1, FRAME_STRIDE):\n",
        "\n",
        "            sub_seq_df = group.iloc[i : i + SAMPLE_LENGTH]\n",
        "\n",
        "            start_frame = sub_seq_df['frame_num'].iloc[0]\n",
        "            end_frame = sub_seq_df['frame_num'].iloc[-1]\n",
        "            real_span = end_frame - start_frame\n",
        "\n",
        "            if real_span > MAX_SPAN_ALLOWED:\n",
        "                continue\n",
        "\n",
        "            seq_meta_data = []\n",
        "\n",
        "            for _, row in sub_seq_df.iterrows():\n",
        "                full_path = row['crop_path_context']\n",
        "                if pd.isna(full_path):\n",
        "                    print(f\"âš ï¸ Warning: missing path for track {track_id} frame {row['frame_num']}\")\n",
        "                    continue\n",
        "\n",
        "                seq_meta_data.append({\n",
        "                    'file_name': os.path.basename(str(full_path)),\n",
        "                    'full_path': str(full_path),\n",
        "                    'frame': row['frame_num'],\n",
        "                    'left': row['left'],\n",
        "                    'top': row['top'],\n",
        "                    'width': row['width'],\n",
        "                    'height': row['height']\n",
        "                })\n",
        "\n",
        "            if len(seq_meta_data) == SAMPLE_LENGTH:\n",
        "                unique_key = f\"{video_id}_{track_id}_{start_frame}\"\n",
        "\n",
        "                inference_samples.append({\n",
        "                    'key': unique_key,\n",
        "                    'video_id': video_id,\n",
        "                    'track_id': track_id,\n",
        "                    'view': view_type,\n",
        "                    'data': seq_meta_data,\n",
        "                    'start_frame': start_frame\n",
        "                })\n",
        "\n",
        "    print(f\"   Total Sequences: {len(inference_samples)}\")\n",
        "\n",
        "    return inference_samples\n",
        "\n",
        "if 'final_consolidated_df' in locals() and not final_consolidated_df.empty:\n",
        "    test_sequences = extract_inference_sequences(final_consolidated_df)\n",
        "\n",
        "    test_samples_endzone = [s for s in test_sequences if s['view'] == 'Endzone']\n",
        "    test_samples_sideline = [s for s in test_sequences if s['view'] == 'Sideline']\n",
        "\n",
        "    print(f\"   - Endzone Samples: {len(test_samples_endzone)}\")\n",
        "    print(f\"   - Sideline Samples: {len(test_samples_sideline)}\")\n",
        "else:\n",
        "    print(\" Error.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aIEUzY_g7zvH",
        "outputId": "7c4fd798-00c0-4376-cf3c-70a3293badbe"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ðŸ•µï¸â€â™‚ï¸ DIAGNOSTICA COMPLETA: DENSITÃ€ E DUPLICATI POSIZIONALI\n",
            "================================================================================\n",
            "â„¹ï¸ Usando colonne coordinate per il check: ['left', 'top', 'width', 'height']\n",
            "\n",
            "ðŸŽ¥ VIDEO: 57906_000718_Endzone\n",
            "   ðŸ“Š DensitÃ :\n",
            "      Media Box/Frame:      20.21\n",
            "      Frame Totali Video:   434\n",
            "      Frame con Dati:       434\n",
            "   ðŸ” Analisi Posizioni:\n",
            "      âœ… NESSUN DUPLICATO: Tutti i box hanno coordinate distinte.\n",
            "      âœ… Nessun frame con 'stacking totale' (tutti su uno).\n",
            "      âœ… STATO: DATI OK\n",
            "\n",
            "ðŸŽ¥ VIDEO: 57906_000718_Sideline\n",
            "   ðŸ“Š DensitÃ :\n",
            "      Media Box/Frame:      18.82\n",
            "      Frame Totali Video:   440\n",
            "      Frame con Dati:       440\n",
            "   ðŸ” Analisi Posizioni:\n",
            "      âœ… NESSUN DUPLICATO: Tutti i box hanno coordinate distinte.\n",
            "      âœ… Nessun frame con 'stacking totale' (tutti su uno).\n",
            "      âœ… STATO: DATI OK\n",
            "\n",
            "ðŸŽ¥ VIDEO: 57995_000109_Endzone\n",
            "   ðŸ“Š DensitÃ :\n",
            "      Media Box/Frame:      19.07\n",
            "      Frame Totali Video:   529\n",
            "      Frame con Dati:       529\n",
            "   ðŸ” Analisi Posizioni:\n",
            "      âœ… NESSUN DUPLICATO: Tutti i box hanno coordinate distinte.\n",
            "      âœ… Nessun frame con 'stacking totale' (tutti su uno).\n",
            "      âœ… STATO: DATI OK\n",
            "\n",
            "ðŸŽ¥ VIDEO: 57995_000109_Sideline\n",
            "   ðŸ“Š DensitÃ :\n",
            "      Media Box/Frame:      26.07\n",
            "      Frame Totali Video:   529\n",
            "      Frame con Dati:       529\n",
            "   ðŸ” Analisi Posizioni:\n",
            "      âœ… NESSUN DUPLICATO: Tutti i box hanno coordinate distinte.\n",
            "      âœ… Nessun frame con 'stacking totale' (tutti su uno).\n",
            "      âœ… STATO: DATI OK\n",
            "\n",
            "ðŸŽ¥ VIDEO: 58102_002798_Endzone\n",
            "   ðŸ“Š DensitÃ :\n",
            "      Media Box/Frame:      16.32\n",
            "      Frame Totali Video:   366\n",
            "      Frame con Dati:       366\n",
            "   ðŸ” Analisi Posizioni:\n",
            "      âœ… NESSUN DUPLICATO: Tutti i box hanno coordinate distinte.\n",
            "      âœ… Nessun frame con 'stacking totale' (tutti su uno).\n",
            "      âœ… STATO: DATI OK\n",
            "\n",
            "ðŸŽ¥ VIDEO: 58102_002798_Sideline\n",
            "   ðŸ“Š DensitÃ :\n",
            "      Media Box/Frame:      24.85\n",
            "      Frame Totali Video:   366\n",
            "      Frame con Dati:       366\n",
            "   ðŸ” Analisi Posizioni:\n",
            "      âœ… NESSUN DUPLICATO: Tutti i box hanno coordinate distinte.\n",
            "      âœ… Nessun frame con 'stacking totale' (tutti su uno).\n",
            "      âœ… STATO: DATI OK\n"
          ]
        }
      ],
      "source": [
        "DF_TO_CHECK = final_consolidated_df \n",
        "VIDEO_DIR = \"/content/datasets/Dataset/test\"\n",
        "\n",
        "def diagnose_helmet_density_and_duplicates(df):\n",
        "\n",
        "    if df.empty:\n",
        "        print(\"âŒ CRITICO: DataFrame vuoto.\")\n",
        "        return\n",
        "\n",
        "    # 1. Standardizzazione Nomi (lavoriamo su una copia per sicurezza)\n",
        "    df_check = df.copy()\n",
        "\n",
        "    # Rinomina frame\n",
        "    if 'frame_num' in df_check.columns:\n",
        "        df_check = df_check.rename(columns={'frame_num': 'frame'})\n",
        "\n",
        "    # Rinomina coordinate in formato standard per il check\n",
        "    rename_map = {'x_min': 'left', 'y_min': 'top', 'x_max': 'right', 'y_max': 'bottom'}\n",
        "    df_check = df_check.rename(columns={k:v for k,v in rename_map.items() if k in df_check.columns})\n",
        "\n",
        "    # Determiniamo quali colonne usare per definire la \"posizione\"\n",
        "    # Di solito bastano left, top, width, height (o right, bottom)\n",
        "    coord_cols = [c for c in ['left', 'top', 'width', 'height', 'right', 'bottom'] if c in df_check.columns]\n",
        "\n",
        "    if not coord_cols:\n",
        "        print(\"âŒ ERRORE: Non trovo colonne di coordinate (left, top, x_min, etc).\")\n",
        "        print(f\"   Colonne disponibili: {df_check.columns.tolist()}\")\n",
        "        return\n",
        "\n",
        "    print(f\"â„¹ï¸ Usando colonne coordinate per il check: {coord_cols}\")\n",
        "\n",
        "    # 2. Loop per Video\n",
        "    unique_videos = df_check['video_id'].unique()\n",
        "\n",
        "    for video_id in unique_videos:\n",
        "        print(f\"\\nðŸŽ¥ VIDEO: {video_id}\")\n",
        "\n",
        "        # Filtriamo i dati\n",
        "        vid_df = df_check[df_check['video_id'] == video_id]\n",
        "\n",
        "        # --- A. CHECK DENSITÃ€ (Conteggio Caschi) ---\n",
        "        frame_counts = vid_df.groupby('frame').size()\n",
        "\n",
        "        avg_helmets = frame_counts.mean()\n",
        "        max_helmets = frame_counts.max()\n",
        "\n",
        "        # --- B. CHECK DUPLICATI POSIZIONALI (Il test richiesto) ---\n",
        "        # Cerchiamo righe che hanno STESSO frame e STESSE coordinate\n",
        "        # keep=False marca TUTTI i duplicati (es. se ho 3 righe uguali, sono 3 duplicati)\n",
        "        subset_cols = ['frame'] + coord_cols\n",
        "        duplicates = vid_df.duplicated(subset=subset_cols, keep=False)\n",
        "        num_dupes = duplicates.sum()\n",
        "\n",
        "        # Analisi approfondita frame per frame\n",
        "        frames_with_stacking = []\n",
        "        for frame, group in vid_df.groupby('frame'):\n",
        "            # Quanti box totali in questo frame?\n",
        "            total_boxes = len(group)\n",
        "            # Quante posizioni UNICHE in questo frame?\n",
        "            unique_positions = group.drop_duplicates(subset=coord_cols).shape[0]\n",
        "\n",
        "            # Se Box Totali > 1 ma Posizioni Uniche == 1, abbiamo lo STACKING TOTALE\n",
        "            if total_boxes > 1 and unique_positions == 1:\n",
        "                frames_with_stacking.append(frame)\n",
        "            # Se ci sono meno posizioni che box, c'Ã¨ sovrapposizione parziale\n",
        "            elif unique_positions < total_boxes:\n",
        "                pass\n",
        "\n",
        "        # --- REPORTING ---\n",
        "\n",
        "        # Info Video Reale\n",
        "        video_path = os.path.join(VIDEO_DIR, video_id + \".mp4\")\n",
        "        cap = cv2.VideoCapture(video_path)\n",
        "        total_frames_vid = \"???\"\n",
        "        if cap.isOpened():\n",
        "            total_frames_vid = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
        "            cap.release()\n",
        "\n",
        "        print(f\"   ðŸ“Š DensitÃ :\")\n",
        "        print(f\"      Media Box/Frame:      {avg_helmets:.2f}\")\n",
        "        print(f\"      Frame Totali Video:   {total_frames_vid}\")\n",
        "        print(f\"      Frame con Dati:       {len(frame_counts)}\")\n",
        "\n",
        "        print(f\"   ðŸ” Analisi Posizioni:\")\n",
        "        if num_dupes == 0:\n",
        "            print(\"      âœ… NESSUN DUPLICATO: Tutti i box hanno coordinate distinte.\")\n",
        "        else:\n",
        "            print(f\"      âš ï¸ TROVATI {num_dupes} BOX CON POSIZIONI DUPLICATE!\")\n",
        "            print(f\"         (Significa che piÃ¹ ID occupano esattamente lo stesso pixel)\")\n",
        "\n",
        "        if len(frames_with_stacking) > 0:\n",
        "            print(f\"      ðŸš¨ BUG STACKING RILEVATO in {len(frames_with_stacking)} frame!\")\n",
        "            print(f\"         Esempio Frame {frames_with_stacking[0]}: Tutti i box sono sovrapposti.\")\n",
        "            print(\"         -> I dati di tracking sono corrotti (coordinate copiate).\")\n",
        "        else:\n",
        "            print(\"      âœ… Nessun frame con 'stacking totale' (tutti su uno).\")\n",
        "\n",
        "        # Verdetto Finale\n",
        "        if len(frames_with_stacking) > 0:\n",
        "            print(\"      âŒ STATO: DATI CORROTTI (STACKING)\")\n",
        "        elif avg_helmets < 5:\n",
        "            print(\"      âš ï¸ STATO: DENSITÃ€ BASSA\")\n",
        "        else:\n",
        "            print(\"      âœ… STATO: DATI OK\")\n",
        "\n",
        "# Esegui\n",
        "if 'final_consolidated_df' in locals():\n",
        "    diagnose_helmet_density_and_duplicates(final_consolidated_df)\n",
        "else:\n",
        "    print(\"âŒ Esegui prima il tracking.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bktcS7-Jes5m"
      },
      "source": [
        "# ***DATASET AND MODEL DEFINITION***"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2Gbz-tyKZUil"
      },
      "outputs": [],
      "source": [
        "def get_inference_transforms():\n",
        "    return T.Compose([\n",
        "        T.ToTensor(),\n",
        "        T.Normalize(mean=[0.43216, 0.394666, 0.37645], std=[0.22803, 0.22145, 0.216989])\n",
        "    ])\n",
        "\n",
        "class NFLInferenceDataset(Dataset):\n",
        "    def __init__(self, samples, root_dir=None, transform=None):\n",
        "      \n",
        "        self.samples = samples\n",
        "        self.root_dir = root_dir\n",
        "        self.transform = transform\n",
        "\n",
        "        \n",
        "        if self.transform is None:\n",
        "            self.transform = get_inference_transforms()\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.samples)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        sample = self.samples[idx]\n",
        "        meta_data = sample['data'] \n",
        "\n",
        "        images = []\n",
        "\n",
        "        for item in meta_data:\n",
        "            \n",
        "            if 'full_path' in item:\n",
        "                img_path = item['full_path']\n",
        "            else:\n",
        "                \n",
        "                vid_folder = item.get('video_folder', '').replace('.mp4', '')\n",
        "                fname = item.get('file_name', '')\n",
        "                if self.root_dir:\n",
        "                    img_path = os.path.join(self.root_dir, vid_folder, fname)\n",
        "                else:\n",
        "                    img_path = fname \n",
        "\n",
        "            try:\n",
        "                \n",
        "                img = Image.open(img_path).convert('RGB')\n",
        "                if img.size != (64, 64):\n",
        "                     img = img.resize((64, 64))\n",
        "\n",
        "            except Exception:\n",
        "                \n",
        "                img = Image.new('RGB', (64, 64), (0, 0, 0))\n",
        "\n",
        "            img_t = self.transform(img)\n",
        "            images.append(img_t)\n",
        "\n",
        "        video_tensor = torch.stack(images).permute(1, 0, 2, 3)\n",
        "\n",
        "        return video_tensor, idx"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-5r05gUvZqH9"
      },
      "outputs": [],
      "source": [
        "class NFLImpactModel(nn.Module):\n",
        "    def __init__(self, num_frames=16, hidden_dim=256, dropout_prob=0.5):\n",
        "        super(NFLImpactModel, self).__init__()\n",
        "\n",
        "        weights = R2Plus1D_18_Weights.DEFAULT\n",
        "        self.backbone = r2plus1d_18(weights=weights)\n",
        "\n",
        "        in_features = self.backbone.fc.in_features\n",
        "        self.backbone.fc = nn.Identity()\n",
        "\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Linear(in_features, hidden_dim),\n",
        "            nn.BatchNorm1d(hidden_dim),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(p=dropout_prob),\n",
        "            nn.Linear(hidden_dim, num_frames)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        features = self.backbone(x)\n",
        "        out = self.classifier(features)\n",
        "        return out\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EDKSRZvuexY5"
      },
      "source": [
        "# ***INFERENCE STEP***"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 465
        },
        "id": "UoTp-6RvZvnD",
        "outputId": "d581977f-f60d-4840-9b2f-198f0a4ccb2b"
      },
      "outputs": [],
      "source": [
        "FINAL_CONFIG = {\n",
        "    'Endzone': {\n",
        "        'thresh': 0.63,       \n",
        "        'ratio': 0.45,        \n",
        "        'nms': 6            \n",
        "    },\n",
        "    'Sideline': {\n",
        "        'thresh': 0.66,       \n",
        "        'ratio': 0.35,        \n",
        "        'nms': 12            \n",
        "    }\n",
        "}\n",
        "\n",
        "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "GT_PATH = \"/content/datasets/Dataset/train_labels.csv\"\n",
        "\n",
        "print(f\" Configuration: {FINAL_CONFIG}\")\n",
        "\n",
        "def run_inference_with_consensus(view_name, samples_list, model_path):\n",
        "\n",
        "    cfg = FINAL_CONFIG[view_name]\n",
        "    threshold = cfg['thresh']\n",
        "    consensus_ratio = cfg['ratio']\n",
        "\n",
        "    print(f\"\\n Inference {view_name} -> Model: {model_path}\")\n",
        "    print(f\"   Parameters -> threshold: {threshold} | Ratio: {consensus_ratio}\")\n",
        "\n",
        "    model = NFLImpactModel(num_frames=16, hidden_dim=256).to(DEVICE)\n",
        "\n",
        "    try:\n",
        "        model.load_state_dict(torch.load(model_path, map_location=DEVICE))\n",
        "    except RuntimeError as e:\n",
        "        print(f\" ERROR: {e}\")\n",
        "        return []\n",
        "\n",
        "    model.eval()\n",
        "\n",
        "    loader = DataLoader(NFLInferenceDataset(samples_list), batch_size=32, shuffle=False)\n",
        "    detection_map = {}\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for video_tensors, indices in tqdm(loader, desc=f\"Predicting {view_name}\"):\n",
        "            video_tensors = video_tensors.to(DEVICE)\n",
        "            probs = torch.sigmoid(model(video_tensors)).cpu().numpy()\n",
        "\n",
        "            for i, prob_vec in enumerate(probs):\n",
        "                sample_meta = samples_list[indices[i].item()]\n",
        "                for off, score in enumerate(prob_vec):\n",
        "                    f_data = sample_meta['data'][off]\n",
        "                    key = (sample_meta['video_id'], sample_meta['track_id'], f_data['frame'])\n",
        "\n",
        "                    if key not in detection_map:\n",
        "                        detection_map[key] = {'s': [], 'm': f_data}\n",
        "                    detection_map[key]['s'].append(score)\n",
        "\n",
        "    refined = []\n",
        "    for (vid, trk, frm), data in detection_map.items():\n",
        "        scores = data['s']\n",
        "        pos_votes = sum(1 for s in scores if s > threshold)\n",
        "        total_votes = len(scores)\n",
        "\n",
        "        if total_votes > 0 and (pos_votes / total_votes) >= consensus_ratio:\n",
        "            parts = vid.split('_')\n",
        "            refined.append({\n",
        "                'gameKey': int(parts[0]),\n",
        "                'playID': int(parts[1]),\n",
        "                'view': parts[2],\n",
        "                'video': f\"{vid}.mp4\",\n",
        "                'frame': frm,\n",
        "                'left': int(data['m']['left']),\n",
        "                'width': int(data['m']['width']),\n",
        "                'top': int(data['m']['top']),\n",
        "                'height': int(data['m']['height']),\n",
        "                'label': f\"Player{trk}\",\n",
        "                'score': float(np.mean(scores))\n",
        "            })\n",
        "\n",
        "    print(f\"   -> Detection: {len(refined)}\")\n",
        "    return refined\n",
        "\n",
        "def apply_asymmetric_nms(df_preds):\n",
        "    if df_preds.empty: return df_preds\n",
        "    print(f\" Using NMS on {len(df_preds)} predictions...\")\n",
        "    final_predictions = []\n",
        "\n",
        "    for view_type in ['Endzone', 'Sideline']:\n",
        "        view_window = FINAL_CONFIG[view_type]['nms']\n",
        "        view_df = df_preds[df_preds['view'] == view_type].copy()\n",
        "        if view_df.empty: continue\n",
        "\n",
        "        grouped = view_df.groupby(['video', 'label'])\n",
        "        for (video, label), group in grouped:\n",
        "            group = group.sort_values('score', ascending=False)\n",
        "            kept_frames = []\n",
        "            for _, row in group.iterrows():\n",
        "                if not any(abs(row['frame'] - kf) < view_window for kf in kept_frames):\n",
        "                    final_predictions.append(row)\n",
        "                    kept_frames.append(row['frame'])\n",
        "\n",
        "    df_final = pd.DataFrame(final_predictions)\n",
        "    if not df_final.empty: df_final = df_final.sort_values(by=['video', 'frame'])\n",
        "    print(f\" NMS Completed: {len(df_preds)} -> {len(df_final)} impatti.\")\n",
        "    return df_final\n",
        "\n",
        "def evaluate_performance_robust(pred_csv, gt_csv):\n",
        "\n",
        "    if isinstance(pred_csv, str): preds = pd.read_csv(pred_csv)\n",
        "    else: preds = pred_csv.copy()\n",
        "\n",
        "    gt = pd.read_csv(gt_csv)\n",
        "\n",
        "    target_videos = [\n",
        "        \"57906_000718_Endzone.mp4\", \"57906_000718_Sideline.mp4\",\n",
        "        \"57995_000109_Endzone.mp4\", \"57995_000109_Sideline.mp4\",\n",
        "        \"58102_002798_Endzone.mp4\", \"58102_002798_Sideline.mp4\"\n",
        "    ]\n",
        "    gt = gt[gt['video'].isin(target_videos) & (gt['impact'] == 1) & (gt['visibility'] > 0)].copy()\n",
        "    gt['matched'] = False\n",
        "\n",
        "    video_results = []\n",
        "    FRAME_TOL = 4\n",
        "    IOU_THRESH = 0.35\n",
        "\n",
        "    for video in target_videos:\n",
        "        v_preds = preds[preds['video'] == video].copy() if not preds.empty else pd.DataFrame()\n",
        "        v_gt = gt[gt['video'] == video].copy()\n",
        "\n",
        "        v_tp = 0\n",
        "        v_fp = 0\n",
        "\n",
        "        if not v_preds.empty:\n",
        "            v_preds = v_preds.sort_values('score', ascending=False)\n",
        "\n",
        "            for _, p in v_preds.iterrows():\n",
        "                candidates = v_gt[\n",
        "                    (v_gt['frame'].between(p['frame'] - FRAME_TOL, p['frame'] + FRAME_TOL)) &\n",
        "                    (v_gt['matched'] == False)\n",
        "                ]\n",
        "\n",
        "                best_iou = 0\n",
        "                best_idx = -1\n",
        "\n",
        "                p_box = [p['left'], p['top'], p['width'], p['height']]\n",
        "\n",
        "                for idx, g in candidates.iterrows():\n",
        "                    g_box = [g['left'], g['top'], g['width'], g['height']]\n",
        "\n",
        "                    xA = max(p_box[0], g_box[0])\n",
        "                    yA = max(p_box[1], g_box[1])\n",
        "                    xB = min(p_box[0]+p_box[2], g_box[0]+g_box[2])\n",
        "                    yB = min(p_box[1]+p_box[3], g_box[1]+g_box[3])\n",
        "\n",
        "                    inter = max(0, xB - xA) * max(0, yB - yA)\n",
        "                    if inter > 0:\n",
        "                        union = (p_box[2]*p_box[3]) + (g_box[2]*g_box[3]) - inter\n",
        "                        iou = inter / union\n",
        "                    else:\n",
        "                        iou = 0\n",
        "\n",
        "                    if iou > best_iou:\n",
        "                        best_iou = iou\n",
        "                        best_idx = idx\n",
        "\n",
        "                if best_iou >= IOU_THRESH:\n",
        "                    v_tp += 1\n",
        "                    v_gt.at[best_idx, 'matched'] = True\n",
        "                else:\n",
        "                    v_fp += 1\n",
        "\n",
        "        v_fn = len(v_gt) - v_tp\n",
        "\n",
        "        denom = (2 * v_tp + v_fp + v_fn)\n",
        "        f1 = (2 * v_tp / denom) if denom > 0 else 0.0\n",
        "\n",
        "        video_results.append({\n",
        "            'Video': video,\n",
        "            'TP': v_tp,\n",
        "            'FP': v_fp,\n",
        "            'FN': v_fn,\n",
        "            'F1': round(f1, 2)\n",
        "        })\n",
        "\n",
        "    res_df = pd.DataFrame(video_results)\n",
        "\n",
        "    if res_df.empty:\n",
        "        return 0.0\n",
        "\n",
        "    print(\"\\n\" + res_df.to_string(index=False))\n",
        "\n",
        "    tp, fp, fn = res_df['TP'].sum(), res_df['FP'].sum(), res_df['FN'].sum()\n",
        "\n",
        "    glob_p = tp / (tp + fp) if (tp + fp) > 0 else 0\n",
        "    glob_r = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
        "    glob_f1 = 2 * glob_p * glob_r / (glob_p + glob_r) if (glob_p + glob_r) > 0 else 0\n",
        "\n",
        "    print(f\"GLOBAL METRICS -> P: {glob_p:.4f} | R: {glob_r:.4f} | F1: {glob_f1:.4f}\")\n",
        "\n",
        "\n",
        "all_preds = []\n",
        "\n",
        "endzone_model_path = \"/content/datasets/Dataset/nfl_r2plus1d_Endzone_BEST_F1.pth\"\n",
        "sideline_model_path = \"/content/datasets/Dataset/nfl_r2plus1d_Sideline_BEST_F1.pth\"\n",
        "\n",
        "\n",
        "if 'test_samples_endzone' in locals() and len(test_samples_endzone) > 0:\n",
        "    all_preds.extend(run_inference_with_consensus('Endzone', test_samples_endzone, endzone_model_path))\n",
        "\n",
        "if 'test_samples_sideline' in locals() and len(test_samples_sideline) > 0:\n",
        "    all_preds.extend(run_inference_with_consensus('Sideline', test_samples_sideline, sideline_model_path))\n",
        "\n",
        "if all_preds:\n",
        "    df_submission = apply_asymmetric_nms(pd.DataFrame(all_preds))\n",
        "    df_submission.to_csv(\"submission.csv\", index=False)\n",
        "    print(\" File 'submission.csv' saved.\")\n",
        "    evaluate_performance_robust(df_submission, GT_PATH)\n",
        "    from google.colab import files\n",
        "    files.download(\"submission.csv\")\n",
        "else:\n",
        "    print(\"Error\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "upKWnXs2ec8q",
        "L2302aeyePHx",
        "Oey0gagceX_p",
        "0IVM2ybwb-YM",
        "Pb4JntuocIX8",
        "bktcS7-Jes5m",
        "hTF9sYkRfUBv"
      ],
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
